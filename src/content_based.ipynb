{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier on a single label using text_tokens as content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import twitter_preproc\n",
    "\n",
    "conf = SparkConf().setAll([\n",
    "    (\"num-executors\", 4), \n",
    "    (\"total-executor-cores\", 16), \n",
    "    (\"executor-memory\", \"8g\"),\n",
    "    (\"spark.yarn.executor.memoryOverhead\", \"64g\")])\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"///tmp/traintweet_10k.tsv\"\n",
    "user = \"engaging_user_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DF and change Timestamp/None to 1/0 in target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "importlib.reload(twitter_preproc)\n",
    "preproc = twitter_preproc.twitter_preproc(spark, sc, datapath, method=\"CB\")\n",
    "df = preproc.getDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle BERT tokens like words, maybe TODO: find deeper meaning in the tokens(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer,NGram,CountVectorizer,IDF,StringIndexer,Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=user, outputCol=user+\"_idx\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text_tokens\", outputCol=\"terms\", pattern=\"\\t\")\n",
    "cv = CountVectorizer(inputCol=\"terms\", outputCol=\"vector\")\n",
    "idf = IDF(inputCol=\"vector\", outputCol=\"features\")\n",
    "normalizer=Normalizer(inputCol=\"features\",outputCol=\"normed_features\")\n",
    "pipeline = Pipeline(stages=[stringIndexer,regexTokenizer, cv,idf,normalizer])\n",
    "\n",
    "model = pipeline.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.transform(df)\n",
    "data = data.select(\"normed_features\",\"engaging_user_id_idx\",\"tweet_id\",\"like_timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Feedback with Rocchios method \n",
    "7, S.36\n",
    "\n",
    "for now with a single user and selected tweeet yeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = \"E7D6C5094767223F6F8789A87A1937AB\"\n",
    "user_id = 1.0\n",
    "features = \"normed_features\"\n",
    "target = \"like_timestamp\"\n",
    "\n",
    "tweet_vector = data.where(data[\"tweet_id\"] == tweet_id)\n",
    "user_vectors = data.where(data[\"engaging_user_id_idx\"] == user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|     normed_features|like_timestamp|\n",
      "+--------------------+--------------+\n",
      "|(31642,[3,4,5,7,1...|    1581004554|\n",
      "|(31642,[0,1,2,3,4...|    1581109718|\n",
      "|(31642,[0,1,3,4,5...|    1581437591|\n",
      "|(31642,[0,1,2,3,4...|    1581182147|\n",
      "|(31642,[0,1,2,3,4...|    1581190598|\n",
      "|(31642,[0,1,2,3,4...|    1580960533|\n",
      "|(31642,[0,1,2,3,4...|    1581530653|\n",
      "|(31642,[0,1,2,3,4...|    1581367528|\n",
      "|(31642,[0,3,4,5,7...|    1581182147|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_vectors.select(\"normed_features\",\"like_timestamp\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate linear combination of user_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback = user_vectors.where(user_vectors[target].isNotNull())\n",
    "negative_feedback = user_vectors.where(user_vectors[target].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     normed_features|\n",
      "+--------------------+\n",
      "|(31642,[3,4,5,7,1...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,3,4,5...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,3,4,5,7...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_feedback.select(\"normed_features\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|normed_features|\n",
      "+---------------+\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negative_feedback.select(\"normed_features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: linear combination of positive - negative and compare the result to the \"to recommend\" tweet, transform result vector anyhow into a percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "#https://stackoverflow.com/questions/46758768/calculating-the-cosine-similarity-between-all-the-rows-of-a-dataframe-in-pyspark\n",
    "\n",
    "mat = IndexedRowMatrix(\n",
    "    data.select(\"engaging_user_id_idx\", \"normed_features\")\\\n",
    "        .rdd.map(lambda row: IndexedRow(row.engaging_user_id_idx, row.normed_features.toArray()))).toBlockMatrix()\n",
    "dot = mat.multiply(mat.transpose())\n",
    "dot.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
