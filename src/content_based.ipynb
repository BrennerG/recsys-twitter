{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based recommender on the twitter dataset\n",
    "\n",
    "We used rocchios algorithm to define the preferences of a user (profile). The user profile is a single feature vector, this vector will be compared with a new observation to determine the cosine similarity. We normalize the cosine similarity with (sim +1)/2 to map it into a range of 0..1. The result is technically not the cosine similarity, but still an indicator if a user likes or dislikes the item.\n",
    "The resulting metric ranges from total negativity(0) to total positivity (1) with 0.5 indicating total independence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import twitter_preproc\n",
    "\n",
    "conf = SparkConf().setAll([\n",
    "    (\"num-executors\", 4), \n",
    "    (\"total-executor-cores\", 32), \n",
    "    (\"executor-memory\", \"8g\"),\n",
    "    (\"spark.yarn.executor.memoryOverhead\", \"64g\")])\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Feedback with Rocchios method \n",
    "\n",
    "https://en.wikipedia.org/wiki/Rocchio_algorithm\n",
    "\n",
    "1. Get every tweet with which a user has interacted\n",
    "2. Split these positive/negativ ones (based on the user interaction).\n",
    "3. Aggregate and average the pos/neg vectors and normalize them 1/N+-\n",
    "5. Multiply each with weights\n",
    "6. Substract the negative feedback from the positive feedback to get a single vector representing the preference of a user\n",
    "\n",
    "This happens in class content_based, in the method generate_user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import RegexTokenizer,NGram,CountVectorizer,IDF,StringIndexer,Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "import scipy.sparse as sps\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class content_based:\n",
    "    def __init__(self, spark: SparkSession, sc: SparkContext):\n",
    "        self.spark = spark\n",
    "        self.sc = sc\n",
    "\n",
    "    \n",
    "    '''\n",
    "    This is a helper method to read a csv file and preprocess using a spark pipeline.\n",
    "    The main purpose in this method is to transform the column text_tokens into an ID-IDF feature vector\n",
    "    This method gets called to process the training set, and the test set in an extra step\n",
    "    '''\n",
    "    def get_processed_data(self,datapath):        \n",
    "        SCHEMA = StructType([\n",
    "            StructField(\"text_tokens\", StringType()),\n",
    "            StructField(\"hashtags\", StringType()),\n",
    "            StructField(\"tweet_id\", StringType()),\n",
    "            StructField(\"present_media\", StringType()),\n",
    "            StructField(\"present_links\", StringType()),\n",
    "            StructField(\"present_domains\", StringType()),\n",
    "            StructField(\"tweet_type\", StringType()),\n",
    "            StructField(\"language\", StringType()),\n",
    "            StructField(\"tweet_timestamp\", LongType()),\n",
    "            StructField(\"engaged_with_user_id\", StringType()),\n",
    "            StructField(\"engaged_with_user_follower_count\", LongType()),\n",
    "            StructField(\"engaged_with_user_following_count\", LongType()),\n",
    "            StructField(\"engaged_with_user_is_verified\", BooleanType()),\n",
    "            StructField(\"engaged_with_user_account_creation\", LongType()),\n",
    "            StructField(\"engaging_user_id\", StringType()),\n",
    "            StructField(\"engaging_user_follower_count\", LongType()),\n",
    "            StructField(\"engaging_user_following_count\", LongType()),\n",
    "            StructField(\"engaging_user_is_verified\", BooleanType()),\n",
    "            StructField(\"engaging_user_account_creation\", LongType()),\n",
    "            StructField(\"engaged_follows_engaging\", BooleanType()),\n",
    "            StructField(\"reply_timestamp\", LongType()),\n",
    "            StructField(\"retweet_timestamp\", LongType()),\n",
    "            StructField(\"retweet_with_comment_timestamp\", LongType()),\n",
    "            StructField(\"like_timestamp\", LongType())       \n",
    "        ])\n",
    "        \n",
    "        raw = spark.read.csv(path=datapath, sep=\"\\x01\", header=False, schema=SCHEMA)\n",
    "        \n",
    "        df = raw.select([\"tweet_id\",\"engaging_user_id\",\n",
    "                                            \"retweet_timestamp\",\"reply_timestamp\",\n",
    "                                            \"retweet_with_comment_timestamp\",\"like_timestamp\",\"text_tokens\"])\n",
    "\n",
    "        for engagement in ENGAGEMENTS:\n",
    "                    df = df.withColumn(engagement, when(df[engagement + \"_timestamp\"].isNotNull(), 1).cast(ByteType()))\\\n",
    "                        .drop(engagement + \"_timestamp\")\n",
    "\n",
    "        df = df.fillna(0, subset=ENGAGEMENTS)\n",
    "\n",
    "        #stringIndexer = StringIndexer(inputCol=\"engaging_user_id\", outputCol=\"engaging_user_id_idx\")\n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"text_tokens\", outputCol=\"terms\", pattern=\"\\t\")\n",
    "        cv = CountVectorizer(inputCol=\"terms\", outputCol=\"vector\")\n",
    "        idf = IDF(inputCol=\"vector\", outputCol=\"features\")\n",
    "        normalizer=Normalizer(inputCol=\"features\",outputCol=\"normed_features\")\n",
    "        pipeline = Pipeline(stages=[regexTokenizer, cv,idf,normalizer])\n",
    "\n",
    "        model = pipeline.fit(df)\n",
    "        data = model.transform(df)\n",
    "        \n",
    "        data = data.select(\"normed_features\",\"tweet_id\",\"engaging_user_id\",\"like\",\"reply\",\"retweet\",\"retweet_with_comment\")\n",
    "\n",
    "        return data\n",
    "    \n",
    "    '''\n",
    "    call this method outside to set the two private variables for training and test\n",
    "    '''\n",
    "    def set_train_test_val(self,trainpath,testpath,valpath):\n",
    "        self.data = self.get_processed_data(trainpath)\n",
    "        self.test = self.get_processed_data(testpath)\n",
    "        self.val = self.get_processed_data(valpath)\n",
    "        \n",
    "    \n",
    "    # \n",
    "    # \n",
    "    '''\n",
    "    https://en.wikipedia.org/wiki/Rocchio_algorithm\n",
    "    This method is only called in the class method get_predictions\n",
    "    Returns a single feature vector representing the users preferences for a single engagement\n",
    "    Gets eventually called 4 times, for each engagement once.\n",
    "    '''\n",
    "    def generate_user_profile(self,engagement):\n",
    "        # transform PySpark Sparse Vectors into scipy csr matrices and generate a paired RDD in form: key:(user_id,0|1 for engagement) value(csr_matrices)\n",
    "        tf = self.data.rdd.map(lambda row: ((row.engaging_user_id,row[engagement]),sps.csr_matrix(row.normed_features)))\n",
    "        \n",
    "        # we saved the key as (user_id,0|1), so we perform a mapreduce on it to generate a linear combination of the positive/negative interaction\n",
    "        # the result from this operation is a RDD, in which each user is 2-times in the set, one row for the linear combination of positives and one row for the negatives\n",
    "        \n",
    "        # How to aggreagate and average \n",
    "        # https://stackoverflow.com/questions/29930110/calculating-the-averages-for-each-key-in-a-pairwise-k-v-rdd-in-spark-with-pyth\n",
    "        aTuple = (0,0)\n",
    "        aggregated = tf.aggregateByKey(aTuple,\n",
    "                                lambda a,b: (a[0] + b,    a[1] + 1),\n",
    "                                lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "        user_vectors = aggregated.mapValues(lambda v: (v[1],v[0]/v[1]))\n",
    "        \n",
    "        # Here we perform the substraction Positive - Negative feedback. The if else lambda is to multiply the negative feedback vector with -1\n",
    "        # with negative values in the negative feedback we can use the associative + operator to perform a substraction\n",
    "        \n",
    "        #tup[0][0] is user_id\n",
    "        #tup[0][1] is the engagement\n",
    "        #tup[1][0] is the count\n",
    "        #tup[1][1] is the feature vectors\n",
    "        \n",
    "        user_profiles = user_vectors.map(lambda tup:(tup[0][0],tup[1][1].multiply(-1)) if tup[0][1] == 0 else (tup[0][0],tup[1][1])).reduceByKey(lambda accumulator,value: accumulator + value)\n",
    "        \n",
    "        \n",
    "        # First we transform the paired RDD into an Row RDD, than to a DataFrame, which is the final output of this method\n",
    "        # the dataframe holds distinct user values, each user holds a single feature vector representing the preferences for a single engagement\n",
    "        \n",
    "        user_profiles_df = user_profiles.map(lambda tup: Row(user_id=tup[0],user_profile = SparseVector(tup[1].shape[1],tup[1].indices,tup[1].data))).toDF()\n",
    "        return user_profiles_df\n",
    "    \n",
    "    '''\n",
    "    Get an RDD in the form user_id,tweet_id,probability for an engagement\n",
    "    '''\n",
    "    def get_predictions(self,engagement):       \n",
    "        user_profile = self.generate_user_profile(engagement)\n",
    "        joined = self.test.join(user_profile,self.test.engaging_user_id==user_profile.user_id)\n",
    "        # We normalize the cosine similarity to map into a range of 0 and 1, (sim + 1) / 2\n",
    "        predictions = joined.rdd.map(lambda row: Row(tweet_id=row.tweet_id,user_id=row.engaging_user_id,probability = ((1.0 - cosine(row.user_profile,row.normed_features).item()) + 1) / 2))\n",
    "        \n",
    "        return predictions.toPandas()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = content_based(spark,sc)\n",
    "\n",
    "#datapath = \"///tmp/traintweet_1000.tsv\"\n",
    "\n",
    "train = \"///tmp/supersecret_train40k.tsv\"\n",
    "testfile = \"///tmp/supersecret_test5k.tsv\"\n",
    "valfile = \"///tmp/supersecret_ensembletrain5k.tsv\"\n",
    "\n",
    "n = 10\n",
    "\n",
    "cb.set_train_test_val(train,testfile,valfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "like = cb.get_predictions(\"like\")\n",
    "reply = cb.get_predictions(\"reply\")\n",
    "retweet = cb.get_predictions(\"retweet\")\n",
    "retweet_with_comment = cb.get_predictions(\"retweet_with_comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cosine_similarity=0.1412570523122424, tweet_id='8CE7533028B0EAEF85EBC4AE37868F91', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.14125705231224228, tweet_id='B3237445D357A16D4637EE178FC7E68C', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='2705449E2CB2E919540B3CCA84286ABE', user_id='00027B86A4118D3249F9352272D53FC1'),\n",
       " Row(cosine_similarity=0.1444346754025927, tweet_id='CD1906D4F3BEC63ECC9547EB1A5523FA', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.1444346754025927, tweet_id='F10E40C008613460549839B51703BCC0', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.20699139018985058, tweet_id='1386298FAE581E8291C62725AF1C0288', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.20247374243819183, tweet_id='37C1FFDF075AD37F81AF15119B446DF9', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.19322785637243367, tweet_id='3D6D0A8601D965D38E54BD4B742F71E8', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A99576B2206B9F093B25AC59CAC21FE0', user_id='0003CBC1EFE332F3534FF13C3A4459CB'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A14F8FE05E2CF0EA52AD664057764FD8', user_id='00062F9C8A42F7DB4447EAE0792C803F')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cosine_similarity=0.1412570523122424, tweet_id='8CE7533028B0EAEF85EBC4AE37868F91', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.14125705231224228, tweet_id='B3237445D357A16D4637EE178FC7E68C', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='2705449E2CB2E919540B3CCA84286ABE', user_id='00027B86A4118D3249F9352272D53FC1'),\n",
       " Row(cosine_similarity=0.14847005825400816, tweet_id='CD1906D4F3BEC63ECC9547EB1A5523FA', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.8515299417459918, tweet_id='F10E40C008613460549839B51703BCC0', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.20699139018985058, tweet_id='1386298FAE581E8291C62725AF1C0288', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.20247374243819183, tweet_id='37C1FFDF075AD37F81AF15119B446DF9', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.19322785637243367, tweet_id='3D6D0A8601D965D38E54BD4B742F71E8', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A99576B2206B9F093B25AC59CAC21FE0', user_id='0003CBC1EFE332F3534FF13C3A4459CB'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A14F8FE05E2CF0EA52AD664057764FD8', user_id='00062F9C8A42F7DB4447EAE0792C803F')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cosine_similarity=0.1412570523122424, tweet_id='8CE7533028B0EAEF85EBC4AE37868F91', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.14125705231224228, tweet_id='B3237445D357A16D4637EE178FC7E68C', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='2705449E2CB2E919540B3CCA84286ABE', user_id='00027B86A4118D3249F9352272D53FC1'),\n",
       " Row(cosine_similarity=0.1444346754025927, tweet_id='CD1906D4F3BEC63ECC9547EB1A5523FA', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.1444346754025927, tweet_id='F10E40C008613460549839B51703BCC0', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.20699139018985058, tweet_id='1386298FAE581E8291C62725AF1C0288', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.20247374243819183, tweet_id='37C1FFDF075AD37F81AF15119B446DF9', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.19322785637243367, tweet_id='3D6D0A8601D965D38E54BD4B742F71E8', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A99576B2206B9F093B25AC59CAC21FE0', user_id='0003CBC1EFE332F3534FF13C3A4459CB'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A14F8FE05E2CF0EA52AD664057764FD8', user_id='00062F9C8A42F7DB4447EAE0792C803F')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet_with_comment.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toCSVLine(data):\n",
    "    return ','.join(str(d)for d in data)\n",
    "\n",
    "lines = like.map(toCSVLine)\n",
    "lines.saveAsTextFile(\"like.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
