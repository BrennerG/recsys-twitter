{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content based recommender on the twitter dataset\n",
    "\n",
    "We used rocchios algorithm to define the preferences of a user (profile). The user profile is a single feature vector, this vector will be compared with a new observation to determine the cosine similarity. We normalize the cosine similarity with (sim +1)/2 to map it into a range of 0..1. The result is technically not the cosine similarity, but still an indicator if a user likes or dislikes the item.\n",
    "The resulting metric ranges from total negativity(0) to total positivity (1) with 0.5 indicating total independence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import twitter_preproc\n",
    "\n",
    "conf = SparkConf().setAll([\n",
    "    (\"num-executors\", 4), \n",
    "    (\"total-executor-cores\", 32), \n",
    "    (\"executor-memory\", \"8g\"),\n",
    "    (\"spark.yarn.executor.memoryOverhead\", \"64g\")])\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Feedback with Rocchios method \n",
    "\n",
    "https://en.wikipedia.org/wiki/Rocchio_algorithm\n",
    "\n",
    "1. Get every tweet with which a user has interacted\n",
    "2. Split these positive/negativ ones (based on the user interaction).\n",
    "3. Aggregate and average the pos/neg vectors and normalize them 1/N+-\n",
    "5. Multiply each with weights\n",
    "6. Substract the negative feedback from the positive feedback to get a single vector representing the preference of a user\n",
    "\n",
    "This happens in class content_based, in the method generate_user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.feature import RegexTokenizer,NGram,CountVectorizer,IDF,StringIndexer,Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "import scipy.sparse as sps\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class content_based:\n",
    "    def __init__(self, spark: SparkSession, sc: SparkContext):\n",
    "        self.spark = spark\n",
    "        self.sc = sc\n",
    "\n",
    "    \n",
    "    '''\n",
    "    This is a helper method to read a csv file and preprocess using a spark pipeline.\n",
    "    The main purpose in this method is to transform the column text_tokens into an ID-IDF feature vector\n",
    "    This method gets called to process the training set, and the test set in an extra step\n",
    "    '''\n",
    "    def get_processed_data(self,datapath):        \n",
    "        SCHEMA = StructType([\n",
    "            StructField(\"text_tokens\", StringType()),\n",
    "            StructField(\"hashtags\", StringType()),\n",
    "            StructField(\"tweet_id\", StringType()),\n",
    "            StructField(\"present_media\", StringType()),\n",
    "            StructField(\"present_links\", StringType()),\n",
    "            StructField(\"present_domains\", StringType()),\n",
    "            StructField(\"tweet_type\", StringType()),\n",
    "            StructField(\"language\", StringType()),\n",
    "            StructField(\"tweet_timestamp\", LongType()),\n",
    "            StructField(\"engaged_with_user_id\", StringType()),\n",
    "            StructField(\"engaged_with_user_follower_count\", LongType()),\n",
    "            StructField(\"engaged_with_user_following_count\", LongType()),\n",
    "            StructField(\"engaged_with_user_is_verified\", BooleanType()),\n",
    "            StructField(\"engaged_with_user_account_creation\", LongType()),\n",
    "            StructField(\"engaging_user_id\", StringType()),\n",
    "            StructField(\"engaging_user_follower_count\", LongType()),\n",
    "            StructField(\"engaging_user_following_count\", LongType()),\n",
    "            StructField(\"engaging_user_is_verified\", BooleanType()),\n",
    "            StructField(\"engaging_user_account_creation\", LongType()),\n",
    "            StructField(\"engaged_follows_engaging\", BooleanType()),\n",
    "            StructField(\"reply_timestamp\", LongType()),\n",
    "            StructField(\"retweet_timestamp\", LongType()),\n",
    "            StructField(\"retweet_with_comment_timestamp\", LongType()),\n",
    "            StructField(\"like_timestamp\", LongType())       \n",
    "        ])\n",
    "        \n",
    "        raw = spark.read.csv(path=datapath, sep=\"\\x01\", header=False, schema=SCHEMA)\n",
    "        \n",
    "        df = raw.select([\"tweet_id\",\"engaging_user_id\",\n",
    "                                            \"retweet_timestamp\",\"reply_timestamp\",\n",
    "                                            \"retweet_with_comment_timestamp\",\"like_timestamp\",\"text_tokens\"])\n",
    "\n",
    "        for engagement in ENGAGEMENTS:\n",
    "                    df = df.withColumn(engagement, when(df[engagement + \"_timestamp\"].isNotNull(), 1).cast(ByteType()))\\\n",
    "                        .drop(engagement + \"_timestamp\")\n",
    "\n",
    "        df = df.fillna(0, subset=ENGAGEMENTS)\n",
    "\n",
    "        #stringIndexer = StringIndexer(inputCol=\"engaging_user_id\", outputCol=\"engaging_user_id_idx\")\n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"text_tokens\", outputCol=\"terms\", pattern=\"\\t\")\n",
    "        cv = CountVectorizer(inputCol=\"terms\", outputCol=\"vector\")\n",
    "        idf = IDF(inputCol=\"vector\", outputCol=\"features\")\n",
    "        normalizer=Normalizer(inputCol=\"features\",outputCol=\"normed_features\")\n",
    "        pipeline = Pipeline(stages=[regexTokenizer, cv,idf,normalizer])\n",
    "\n",
    "        model = pipeline.fit(df)\n",
    "        data = model.transform(df)\n",
    "        \n",
    "        data = data.select(\"normed_features\",\"tweet_id\",\"engaging_user_id\",\"like\",\"reply\",\"retweet\",\"retweet_with_comment\")\n",
    "\n",
    "        return data\n",
    "    \n",
    "    '''\n",
    "    call this method outside to set the two private variables for training and test\n",
    "    '''\n",
    "    def set_train_test_val(self,trainpath,testpath,valpath):\n",
    "        self.data = self.get_processed_data(trainpath)\n",
    "        self.test = self.get_processed_data(testpath)\n",
    "        self.val = self.get_processed_data(valpath)\n",
    "        \n",
    "    \n",
    "    # \n",
    "    # \n",
    "    '''\n",
    "    https://en.wikipedia.org/wiki/Rocchio_algorithm\n",
    "    This method is only called in the class method get_predictions\n",
    "    Returns a single feature vector representing the users preferences for a single engagement\n",
    "    Gets eventually called 4 times, for each engagement once.\n",
    "    '''\n",
    "    def generate_user_profile(self,engagement):\n",
    "        # transform PySpark Sparse Vectors into scipy csr matrices and generate a paired RDD in form: key:(user_id,0|1 for engagement) value(csr_matrices)\n",
    "        tf = self.data.rdd.map(lambda row: ((row.engaging_user_id,row[engagement]),sps.csr_matrix(row.normed_features)))\n",
    "        \n",
    "        # we saved the key as (user_id,0|1), so we perform a mapreduce on it to generate a linear combination of the positive/negative interaction\n",
    "        # the result from this operation is a RDD, in which each user is 2-times in the set, one row for the linear combination of positives and one row for the negatives\n",
    "        \n",
    "        # How to aggreagate and average \n",
    "        # https://stackoverflow.com/questions/29930110/calculating-the-averages-for-each-key-in-a-pairwise-k-v-rdd-in-spark-with-pyth\n",
    "        aTuple = (0,0)\n",
    "        aggregated = tf.aggregateByKey(aTuple,\n",
    "                                lambda a,b: (a[0] + b,    a[1] + 1),\n",
    "                                lambda a,b: (a[0] + b[0], a[1] + b[1]))\n",
    "        user_vectors = aggregated.mapValues(lambda v: (v[1],v[0]/v[1]))\n",
    "        \n",
    "        # Here we perform the substraction Positive - Negative feedback. The if else lambda is to multiply the negative feedback vector with -1\n",
    "        # with negative values in the negative feedback we can use the associative + operator to perform a substraction\n",
    "        \n",
    "        #tup[0][0] is user_id\n",
    "        #tup[0][1] is the engagement\n",
    "        #tup[1][0] is the count\n",
    "        #tup[1][1] is the feature vectors\n",
    "        \n",
    "        user_profiles = user_vectors.map(lambda tup:(tup[0][0],tup[1][1].multiply(-1)) if tup[0][1] == 0 else (tup[0][0],tup[1][1])).reduceByKey(lambda accumulator,value: accumulator + value)\n",
    "        \n",
    "        \n",
    "        # First we transform the paired RDD into an Row RDD, than to a DataFrame, which is the final output of this method\n",
    "        # the dataframe holds distinct user values, each user holds a single feature vector representing the preferences for a single engagement\n",
    "        \n",
    "        user_profiles_df = user_profiles.map(lambda tup: Row(user_id=tup[0],user_profile = SparseVector(tup[1].shape[1],tup[1].indices,tup[1].data))).toDF()\n",
    "        return user_profiles_df\n",
    "    \n",
    "    '''\n",
    "    Get an RDD in the form user_id,tweet_id,probability for an engagement\n",
    "    '''\n",
    "    def get_predictions(self,engagement):       \n",
    "        user_profile = self.generate_user_profile(engagement)\n",
    "        joined = self.test.join(user_profile,self.test.engaging_user_id==user_profile.user_id)\n",
    "        # We normalize the cosine similarity to map into a range of 0 and 1, (sim + 1) / 2\n",
    "        predictions = joined.rdd.map(lambda row: Row(tweet_id=row.tweet_id,user_id=row.engaging_user_id,probability = ((1.0 - cosine(row.user_profile,row.normed_features).item()) + 1) / 2))\n",
    "        \n",
    "        return predictions.toPandas()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = content_based(spark,sc)\n",
    "\n",
    "datapath = \"///tmp/traintweet_1000.tsv\"\n",
    "ENGAGEMENTS = [\"like\", \"reply\", \"retweet\", \"retweet_with_comment\"]\n",
    "\n",
    "train = \"///tmp/supersecret_train40k.tsv\"\n",
    "testfile = \"///tmp/supersecret_test5k.tsv\"\n",
    "valfile = \"///tmp/supersecret_ensembletrain5k.tsv\"\n",
    "\n",
    "cb.set_train_test_val(train,testfile,valfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like = cb.get_predictions(\"like\")\n",
    "like.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "like = cb.get_predictions(\"like\")\n",
    "reply = cb.get_predictions(\"reply\")\n",
    "retweet = cb.get_predictions(\"retweet\")\n",
    "retweet_with_comment = cb.get_predictions(\"retweet_with_comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "like.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: \nAborting TaskSet 259.0 because task 2 (partition 2)\ncannot run anywhere due to node and executor blacklist.\nMost recent failure:\nLost task 2.1 in stage 259.0 (TID 579, c106.local, executor 19): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1055, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1055, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-59-e4abc41acf6f>\", line 135, in <lambda>\n  File \"/home/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py\", line 744, in cosine\n    return correlation(u, v, w=w, centered=False)\n  File \"/home/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py\", line 695, in correlation\n    uv = np.average(u * v, weights=w)\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2121)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2121)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nBlacklisting behavior can be configured via spark.blacklist.*.\n\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2146)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-b3fd2bf17957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \"\"\"\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \"\"\"\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: \nAborting TaskSet 259.0 because task 2 (partition 2)\ncannot run anywhere due to node and executor blacklist.\nMost recent failure:\nLost task 2.1 in stage 259.0 (TID 579, c106.local, executor 19): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 2499, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 352, in func\n    return f(iterator)\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1055, in <lambda>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/home/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\", line 1055, in <genexpr>\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-59-e4abc41acf6f>\", line 135, in <lambda>\n  File \"/home/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py\", line 744, in cosine\n    return correlation(u, v, w=w, centered=False)\n  File \"/home/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py\", line 695, in correlation\n    uv = np.average(u * v, weights=w)\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'float'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2121)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2121)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$11.apply(Executor.scala:407)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1408)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:413)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\nBlacklisting behavior can be configured via spark.blacklist.*.\n\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1890)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:929)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:929)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2111)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2049)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:740)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2081)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2102)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2121)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2146)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "like.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cosine_similarity=0.1412570523122424, tweet_id='8CE7533028B0EAEF85EBC4AE37868F91', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.14125705231224228, tweet_id='B3237445D357A16D4637EE178FC7E68C', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='2705449E2CB2E919540B3CCA84286ABE', user_id='00027B86A4118D3249F9352272D53FC1'),\n",
       " Row(cosine_similarity=0.1444346754025927, tweet_id='CD1906D4F3BEC63ECC9547EB1A5523FA', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.1444346754025927, tweet_id='F10E40C008613460549839B51703BCC0', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.20699139018985058, tweet_id='1386298FAE581E8291C62725AF1C0288', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.20247374243819183, tweet_id='37C1FFDF075AD37F81AF15119B446DF9', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.19322785637243367, tweet_id='3D6D0A8601D965D38E54BD4B742F71E8', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A99576B2206B9F093B25AC59CAC21FE0', user_id='0003CBC1EFE332F3534FF13C3A4459CB'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A14F8FE05E2CF0EA52AD664057764FD8', user_id='00062F9C8A42F7DB4447EAE0792C803F')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cosine_similarity=0.1412570523122424, tweet_id='8CE7533028B0EAEF85EBC4AE37868F91', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.14125705231224228, tweet_id='B3237445D357A16D4637EE178FC7E68C', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='2705449E2CB2E919540B3CCA84286ABE', user_id='00027B86A4118D3249F9352272D53FC1'),\n",
       " Row(cosine_similarity=0.14847005825400816, tweet_id='CD1906D4F3BEC63ECC9547EB1A5523FA', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.8515299417459918, tweet_id='F10E40C008613460549839B51703BCC0', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.20699139018985058, tweet_id='1386298FAE581E8291C62725AF1C0288', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.20247374243819183, tweet_id='37C1FFDF075AD37F81AF15119B446DF9', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.19322785637243367, tweet_id='3D6D0A8601D965D38E54BD4B742F71E8', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A99576B2206B9F093B25AC59CAC21FE0', user_id='0003CBC1EFE332F3534FF13C3A4459CB'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A14F8FE05E2CF0EA52AD664057764FD8', user_id='00062F9C8A42F7DB4447EAE0792C803F')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cosine_similarity=0.1412570523122424, tweet_id='8CE7533028B0EAEF85EBC4AE37868F91', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.14125705231224228, tweet_id='B3237445D357A16D4637EE178FC7E68C', user_id='00007F67532643E55C33DA326FE0FB5C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='2705449E2CB2E919540B3CCA84286ABE', user_id='00027B86A4118D3249F9352272D53FC1'),\n",
       " Row(cosine_similarity=0.1444346754025927, tweet_id='CD1906D4F3BEC63ECC9547EB1A5523FA', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.1444346754025927, tweet_id='F10E40C008613460549839B51703BCC0', user_id='0002A3D7A02C1D269FD136500342C274'),\n",
       " Row(cosine_similarity=0.20699139018985058, tweet_id='1386298FAE581E8291C62725AF1C0288', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.20247374243819183, tweet_id='37C1FFDF075AD37F81AF15119B446DF9', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.19322785637243367, tweet_id='3D6D0A8601D965D38E54BD4B742F71E8', user_id='0002B7B662BC907411052C8884FB347C'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A99576B2206B9F093B25AC59CAC21FE0', user_id='0003CBC1EFE332F3534FF13C3A4459CB'),\n",
       " Row(cosine_similarity=0.0, tweet_id='A14F8FE05E2CF0EA52AD664057764FD8', user_id='00062F9C8A42F7DB4447EAE0792C803F')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retweet_with_comment.take(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toCSVLine(data):\n",
    "    return ','.join(str(d)for d in data)\n",
    "\n",
    "lines = like.map(toCSVLine)\n",
    "lines.saveAsTextFile(\"like.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
