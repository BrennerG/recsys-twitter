{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier on a single label using text_tokens as content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import twitter_preproc\n",
    "\n",
    "conf = SparkConf().setAll([\n",
    "    (\"num-executors\", 4), \n",
    "    (\"total-executor-cores\", 16), \n",
    "    (\"executor-memory\", \"8g\"),\n",
    "    (\"spark.yarn.executor.memoryOverhead\", \"64g\")])\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"///tmp/traintweet_10k.tsv\"\n",
    "ENGAGEMENTS = [\"like\", \"reply\", \"retweet\", \"retweet_with_comment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DF and change Timestamp/None to 1/0 in target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "importlib.reload(twitter_preproc)\n",
    "preproc = twitter_preproc.twitter_preproc(spark, sc, datapath, MF=True)\n",
    "df = preproc.getDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "for engagement in ENGAGEMENTS:\n",
    "            df = df.withColumn(engagement, when(df[engagement + \"_timestamp\"].isNotNull(), 1).cast(ByteType()))\\\n",
    "                .drop(engagement + \"_timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0, subset=ENGAGEMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(\"text_tokens\",\"tweet_id\",\"engaging_user_id\",\"like\",\"reply\",\"retweet\",\"retweet_with_comment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle BERT tokens like words, maybe TODO: find deeper meaning in the tokens(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer,NGram,CountVectorizer,IDF,StringIndexer,Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"engaging_user_id\", outputCol=\"engaging_user_id_idx\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text_tokens\", outputCol=\"terms\", pattern=\"\\t\")\n",
    "cv = CountVectorizer(inputCol=\"terms\", outputCol=\"vector\")\n",
    "idf = IDF(inputCol=\"vector\", outputCol=\"features\")\n",
    "normalizer=Normalizer(inputCol=\"features\",outputCol=\"normed_features\")\n",
    "pipeline = Pipeline(stages=[stringIndexer,regexTokenizer, cv,idf,normalizer])\n",
    "\n",
    "model = pipeline.fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text_tokens: string (nullable = true)\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- engaging_user_id: string (nullable = true)\n",
      " |-- like: byte (nullable = true)\n",
      " |-- reply: byte (nullable = true)\n",
      " |-- retweet: byte (nullable = true)\n",
      " |-- retweet_with_comment: byte (nullable = true)\n",
      " |-- engaging_user_id_idx: double (nullable = false)\n",
      " |-- terms: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- vector: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- normed_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance Feedback with Rocchios method \n",
    "7, S.36\n",
    "\n",
    "for now with a single user and selected tweeet yeet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = \"E7D6C5094767223F6F8789A87A1937AB\"\n",
    "user_id = 1.0\n",
    "features = \"normed_features\"\n",
    "target = \"like_timestamp\"\n",
    "\n",
    "tweet_vector = data.where(data[\"tweet_id\"] == tweet_id)\n",
    "user_vectors = data.where(data[\"engaging_user_id_idx\"] == user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+-------+--------------------+\n",
      "|     normed_features|like|reply|retweet|retweet_with_comment|\n",
      "+--------------------+----+-----+-------+--------------------+\n",
      "|(31642,[3,4,5,7,1...|   1|    0|      0|                   0|\n",
      "|(31642,[0,1,2,3,4...|   1|    0|      0|                   0|\n",
      "|(31642,[0,1,3,4,5...|   1|    0|      0|                   0|\n",
      "|(31642,[0,1,2,3,4...|   1|    0|      0|                   0|\n",
      "|(31642,[0,1,2,3,4...|   1|    0|      0|                   0|\n",
      "|(31642,[0,1,2,3,4...|   1|    0|      0|                   0|\n",
      "|(31642,[0,1,2,3,4...|   1|    0|      0|                   0|\n",
      "|(31642,[0,1,2,3,4...|   1|    0|      0|                   0|\n",
      "|(31642,[0,3,4,5,7...|   1|    0|      0|                   0|\n",
      "+--------------------+----+-----+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_vectors.select(\"normed_features\",\"like\",\"reply\",\"retweet\",\"retweet_with_comment\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate linear combination of user_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_feedback = user_vectors.where(user_vectors[\"like\"].isNotNull())\n",
    "negative_feedback = user_vectors.where(user_vectors[\"like\"].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     normed_features|\n",
      "+--------------------+\n",
      "|(31642,[3,4,5,7,1...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,3,4,5...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,1,2,3,4...|\n",
      "|(31642,[0,3,4,5,7...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_feedback.select(\"normed_features\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|normed_features|\n",
      "+---------------+\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negative_feedback.select(\"normed_features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "def vec2array(v):\n",
    "  v = Vectors.dense(v)\n",
    "  array = list([float(x) for x in v])\n",
    "  return array\n",
    "\n",
    "vec2array_udf = F.udf(vec2array, T.ArrayType(T.FloatType()))\n",
    "\n",
    "user_profile = user_vectors.withColumn('normed_features', vec2array_udf('normed_features'))\n",
    "\n",
    "n = len(user_profile.select('normed_features').first()[0])\n",
    "d_plus = user_profile.agg(F.array(*[F.sum(F.col(\"normed_features\")[i]) for i in range(n)]).alias(\"sum\"))\n",
    "d_plus.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: linear combination of positive - negative and compare the result to the \"to recommend\" tweet, transform result vector anyhow into a percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "#https://stackoverflow.com/questions/46758768/calculating-the-cosine-similarity-between-all-the-rows-of-a-dataframe-in-pyspark\n",
    "\n",
    "mat = IndexedRowMatrix(\n",
    "    data.select(\"engaging_user_id_idx\", \"normed_features\")\\\n",
    "        .rdd.map(lambda row: IndexedRow(row.engaging_user_id_idx, row.normed_features.toArray()))).toBlockMatrix()\n",
    "dot = mat.multiply(mat.transpose())\n",
    "dot.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Based approach\n",
    "\n",
    "Our approach was to treat the BERT tokens like words and generate an id-idf feature vector. Every tweet is represented as a sparse feature vector, the user profile is generated with every tweet which the user has engaged with. \n",
    "\n",
    "We tried to use Rocchio's Method to generate relevance feedback for a user. This method was not feasable, due to the massive amount of features. For Rocchio's Method past rated items are split into two classes, positive feedback and negative feedback. To compute the user profile, one has to aggregate each feature vector from the positive and negative ones. This step was crucial for performance, because the aggreation of a sparse vector with ~32.000 features did not finish in a reasonable time.\n",
    "\n",
    "## Vector aggregation in pyspark\n",
    "\n",
    "It is neccessary in pyspark to transform a SparseVector into an intermediate format, in this case an array, to perform aggregate functions. So for the linear combination of our user feedback, we had to aggregate arrays instead of SparseVectors, which is also a factor for the poor performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
