{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "import twitter_preproc\n",
    "import importlib\n",
    "importlib.reload(twitter_preproc)\n",
    "from twitter_preproc import *\n",
    "from operator import attrgetter\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from twitter_preproc import twitter_preproc\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys, traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Spark Context\n",
    "# conf = SparkConf().setAll([('spark.executor.memory', '32g'), ('spark.executor.instances','8'),('spark.executor.cores', '12'), ('spark.driver.memory','64g'), ('spark.driver.memoryOverhead', '64g')])\n",
    "conf = SparkConf()\n",
    "spark = SparkSession.builder.appName(\"nncf_train\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASIC PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "\n",
    "# Small datasets\n",
    "# train = \"///tmp/traintweet_1000.tsv\"\n",
    "train = \"///tmp/traintweet_10k.tsv\"\n",
    "# train = \"/tmp/supersecret_train40k_bootstrap.tsv\"\n",
    "\n",
    "preproc = twitter_preproc(spark, sc, train, MF=True)\n",
    "traindata = preproc.getDF()\n",
    "traindata = traindata.limit(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN PREPROCESSING (INDEXING, ONE-HOTTING, MATRIX CONVERSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%file NNPreprocessor.py\n",
    "import pyspark.sql.functions as Fun\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import vstack\n",
    "import twitter_preproc\n",
    "import importlib\n",
    "importlib.reload(twitter_preproc)\n",
    "from twitter_preproc import *\n",
    "from operator import attrgetter\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from twitter_preproc import twitter_preproc\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "import torch\n",
    "import sys, traceback\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NNPreprocessor:\n",
    "\n",
    "    def get_id_indices(self,df, id_column):\n",
    "        id_indices = df.select(id_column).orderBy(id_column).rdd.zipWithIndex().toDF()\n",
    "        id_indices = id_indices.withColumn(id_column, Fun.col(\"_1\")[id_column])\\\n",
    "            .select(Fun.col(id_column), Fun.col(\"_2\").alias(id_column + \"_index\"))\n",
    "        return id_indices\n",
    "\n",
    "    # map function to convert from spark vectors to sparse numpy csr matrix\n",
    "    def as_matrix(self,vec):\n",
    "        data, indices = vec.values, vec.indices\n",
    "        shape = 1, vec.size\n",
    "        return csr_matrix((data, indices, np.array([0, vec.values.size])), shape)\n",
    "\n",
    "    def get_pytorch_sparse(self, attr, traindata_ohe):\n",
    "        features = traindata_ohe.rdd.map(attrgetter(attr))\n",
    "        mats = features.map(lambda vec: csr_matrix((vec.values, vec.indices, np.array([0, vec.values.size])), (1,vec.size)))\n",
    "        mat = mats.reduce(lambda x, y: vstack([x, y]))\n",
    "        return mat\n",
    "\n",
    "    # convert to pytorch format\n",
    "    def transform_to_sparse_tensor(self,data):\n",
    "        coo = data.tocoo()\n",
    "        values = coo.data\n",
    "        indices = np.vstack((coo.row, coo.col))\n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = coo.shape\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "    def nn_preprocess(self, data, engagement='like'):\n",
    "        traindata = data\n",
    "\n",
    "        # INDEXING & ONE-HOT ENCODING\n",
    "        # get indexed columns\n",
    "        tweet_id_idx = self.get_id_indices(df=traindata, id_column=\"tweet_id\")\n",
    "        user_id_idx = self.get_id_indices(df=traindata, id_column=\"engaging_user_id\")\n",
    "        # rejoin the columns\n",
    "        indexed_data = traindata.join(tweet_id_idx, ['tweet_id']).join(user_id_idx, ['engaging_user_id'])\n",
    "\n",
    "        # one-hot-encode\n",
    "        pipeline = Pipeline(stages=[\n",
    "            OneHotEncoder(inputCol=\"tweet_id_index\",  outputCol=\"tweet_id_ohe\"),\n",
    "            OneHotEncoder(inputCol=\"engaging_user_id_index\",  outputCol=\"user_id_ohe\")\n",
    "        ])\n",
    "        model = pipeline.fit(indexed_data.select(['tweet_id_index', 'engaging_user_id_index', engagement]))\n",
    "        traindata_ohe = model.transform(indexed_data)\n",
    "\n",
    "        # select and parse to pandas dataframe\n",
    "        df = pd.DataFrame(traindata_ohe.select(['tweet_id_ohe', 'user_id_ohe', engagement]).collect(), columns=['tweet_id_ohe', 'user_id_ohe', engagement])\n",
    "\n",
    "        # create tweets and users vector in correct format    \n",
    "        tweet_sparse = self.get_pytorch_sparse(\"tweet_id_ohe\", traindata_ohe)\n",
    "        user_sparse = self.get_pytorch_sparse(\"user_id_ohe\", traindata_ohe)\n",
    "        tweets = self.transform_to_sparse_tensor(tweet_sparse).to_dense()\n",
    "        users = self.transform_to_sparse_tensor(user_sparse).to_dense()\n",
    "        \n",
    "        # create target variables in correct format\n",
    "        y = torch.FloatTensor(traindata_ohe.select(\"like\").collect()) \n",
    "        target = y  \n",
    "        target = target.view(1, -1).t()\n",
    "\n",
    "        return tweets, users, target\n",
    "    \n",
    "    def pad(self, tweets, users, target, dim):  \n",
    "        padding_users = dim - users.shape[1] - 1\n",
    "        padding_tweets = dim - tweets.shape[1] - 1\n",
    "        padding_target = users.shape[0] - target.shape[0]\n",
    "        padded_users = F.pad(users, (padding_users,1))\n",
    "        padded_tweets = F.pad(tweets, (padding_tweets,1))\n",
    "        padded_target = F.pad(target, (1, padding_target))\n",
    "        return padded_tweets, padded_users, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4427, 5000]), torch.Size([4427, 5000]), torch.Size([4427, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnp = NNPreprocessor()\n",
    "unpadded_tweets, unpadded_users, unpadded_target = nnp.nn_preprocess(traindata)\n",
    "tweets, users, target = nnp.pad(unpadded_tweets, unpadded_users, unpadded_target, 5000)\n",
    "tweets.shape, users.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%file NNCFNet.py\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Neural Network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, users:int, items:int, k:int):\n",
    "        super(Net, self).__init__()\n",
    "        self.dense1 = nn.Linear(users, k)\n",
    "        self.dense2 = nn.Linear(items, k)\n",
    "        self.fc1 = nn.Linear(2*k, k)\n",
    "        self.fc2 = nn.Linear(k, math.floor(k/2))\n",
    "        self.fc3 = nn.Linear(math.floor(k/2), 1)\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        users = F.relu(self.dense1(users))\n",
    "        items = F.relu(self.dense2(items))\n",
    "        # concat users and items into 1 vector\n",
    "        x = torch.cat((users, items), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        sigfried = nn.Sigmoid()\n",
    "        x = sigfried(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Training\n",
      "epoch  1\n",
      "epoch  2\n",
      "epoch  3\n",
      "epoch  4\n",
      "epoch  5\n"
     ]
    }
   ],
   "source": [
    "from NNCFNet import Net\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Initalize Hyperparameters\n",
    "k = 32\n",
    "n_epochs = 5\n",
    "batch_size = 10\n",
    "\n",
    "# Initialize Neural Network\n",
    "net = Net(users.shape[1], tweets.shape[1], k)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "output = net(users, tweets)\n",
    "\n",
    "# Start training\n",
    "print(\"\\nStart Training\")\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"epoch \", epoch+1)\n",
    "\n",
    "    try:\n",
    "    # X is a torch Variable\n",
    "        permutation = torch.randperm(users.size()[0])\n",
    "\n",
    "        for i in range(0,users.size()[0], batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            indices = permutation[i:i+batch_size]\n",
    "            batch_x_user = users[indices]\n",
    "            batch_x_tweet = tweets[indices]\n",
    "            batch_y = target[indices]\n",
    "\n",
    "            outputs = net.forward(batch_x_user, batch_x_tweet)\n",
    "            loss = criterion(outputs,batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print(loss)\n",
    "\n",
    "    except:\n",
    "        traceback.print_stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "PATH = '../misc/NNCF_model_save_.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "print(\"\\n\\nDONE. model saved to \", PATH, \"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
