{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import importlib\n",
    "\n",
    "# Building Spark Context\n",
    "# conf = SparkConf().setAll([('spark.executor.memory', '32g'), ('spark.executor.instances','8'),('spark.executor.cores', '12'), ('spark.driver.memory','64g'), ('spark.driver.memoryOverhead', '64g')])\n",
    "conf = SparkConf()\n",
    "spark = SparkSession.builder.appName(\"nncf_train\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_preproc\n",
    "\n",
    "base = \"///tmp/\"\n",
    "one_k = \"traintweet_1000.tsv\"\n",
    "ensemble_train = 'supersecret_ensembletrain5k_bootstrap.tsv'\n",
    "ensemble_test = 'supersecret_test5k_bootstrap.tsv'\n",
    "choice = ensemble_train\n",
    "\n",
    "preproc = twitter_preproc.twitter_preproc(spark, sc, base+choice, MF=True)\n",
    "traindata = preproc.getDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnpreprocessor\n",
    "importlib.reload(nnpreprocessor)\n",
    "\n",
    "nnp = nnpreprocessor.NNPreprocessor()\n",
    "engagement = 'retweet_comment'\n",
    "tweets, users, target = nnp.nn_preprocess(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start Training\n",
      "epoch  1\n",
      "tensor(0.6872, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6823, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6917, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6833, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6863, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6868, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6858, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6878, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6868, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6892, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6833, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6838, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6927, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6848, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6942, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6912, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6793, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6833, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6927, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6896, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "epoch  2\n",
      "tensor(0.6877, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6872, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6862, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6917, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6798, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6857, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6867, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6942, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6902, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6798, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6877, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6897, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6867, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6887, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6847, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6823, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6902, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6872, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "tensor(0.6911, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "from NNCFNet import Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sys, traceback\n",
    "\n",
    "# Initalize Hyperparameters\n",
    "k = 32\n",
    "n_epochs = 2\n",
    "batch_size = 256\n",
    "\n",
    "# Initialize Neural Network\n",
    "net = Net(users.shape[1], tweets.shape[1], k)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "output = net(users, tweets)\n",
    "\n",
    "# Start training\n",
    "print(\"\\nStart Training\")\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"epoch \", epoch+1)\n",
    "\n",
    "    permutation = torch.randperm(users.size()[0])\n",
    "\n",
    "    for i in range(0,users.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x_user = users[indices]\n",
    "        batch_x_tweet = tweets[indices]\n",
    "        batch_y = target[indices]\n",
    "\n",
    "        outputs = net.forward(batch_x_user, batch_x_tweet)\n",
    "        loss = criterion(outputs,batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RE-CREATE INPUT FORMAT & ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import numpy as np\n",
    "\n",
    "# get predictions\n",
    "net.eval()\n",
    "prediction = net(users, tweets)\n",
    "p_vec = prediction.detach().numpy().flatten()\n",
    "scaled = (p_vec - np.min(p_vec))/np.ptp(p_vec)\n",
    "probabilities = [float(x) for x in scaled]\n",
    "\n",
    "# establish original order\n",
    "order_df = traindata.withColumn(\"original_order\", monotonically_increasing_id())\n",
    "order_df = order_df.select(\"engaging_user_id\", \"tweet_id\", 'original_order')\n",
    "sorting_tweets = nnp.get_id_indices(order_df, id_column='tweet_id')\n",
    "\n",
    "# join labels\n",
    "result = order_df.join(sorting_tweets, 'tweet_id').sort('original_order').rdd.map(lambda x: (x['engaging_user_id'], x['tweet_id'], probabilities[x['tweet_id_index']], x['original_order']))\n",
    "\n",
    "# ensure order\n",
    "result_df = spark.createDataFrame(result).toDF('engaging_user_id', 'tweet_id', 'target', 'original_order')\n",
    "clean = result_df.dropDuplicates(['engaging_user_id', 'tweet_id']).sort('original_order').select('engaging_user_id', 'tweet_id', 'target').rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_PATH = engagement + '.' + choice\n",
    "\n",
    "def toCSVLine(data):\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "lines = clean.map(toCSVLine)\n",
    "lines.saveAsTextFile(output_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
