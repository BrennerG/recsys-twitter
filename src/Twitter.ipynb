{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter RecSys Challenge 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from twitter_preproc import twitter_preproc\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"ChiSquareSpark\").getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproc Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting twitter_preproc.py\n"
     ]
    }
   ],
   "source": [
    "%%file twitter_preproc.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import * \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import RegexTokenizer, OneHotEncoderEstimator, StringIndexer, MinMaxScaler, VectorAssembler, HashingTF, IDF\n",
    "\n",
    "class twitter_preproc:\n",
    "    \n",
    "    def __init__(self, spark:SparkSession, sc:SparkContext, trainFile:str, testFile:str=\"\", seed:int=123,\n",
    "                 MF:bool=False, trainsplit:float=0.9):\n",
    "        \n",
    "        self.sc = sc\n",
    "        #inputRDD = sc.textFile(inputFile)\n",
    "        #self.inputData = spark.read.option(\"sep\", \"\\x01\").csv(inputFile)\n",
    "        SCHEMA = StructType([\n",
    "                StructField(\"text_tokens\", StringType()),\n",
    "                StructField(\"hashtags\", StringType()),\n",
    "                StructField(\"tweet_id\", StringType()),\n",
    "                StructField(\"present_media\", StringType()),\n",
    "                StructField(\"present_links\", StringType()),\n",
    "                StructField(\"present_domains\", StringType()),\n",
    "                StructField(\"tweet_type\", StringType()),\n",
    "                StructField(\"language\", StringType()),\n",
    "                StructField(\"tweet_timestamp\", LongType()),\n",
    "                StructField(\"engaged_with_user_id\", StringType()),\n",
    "                StructField(\"engaged_with_user_follower_count\", LongType()),\n",
    "                StructField(\"engaged_with_user_following_count\", LongType()),\n",
    "                StructField(\"engaged_with_user_is_verified\", BooleanType()),\n",
    "                StructField(\"engaged_with_user_account_creation\", LongType()),\n",
    "                StructField(\"engaging_user_id\", StringType()),\n",
    "                StructField(\"engaging_user_follower_count\", LongType()),\n",
    "                StructField(\"engaging_user_following_count\", LongType()),\n",
    "                StructField(\"engaging_user_is_verified\", BooleanType()),\n",
    "                StructField(\"engaging_user_account_creation\", LongType()),\n",
    "                StructField(\"engaged_follows_engaging\", BooleanType()),\n",
    "                StructField(\"reply_timestamp\", LongType()),\n",
    "                StructField(\"retweet_timestamp\", LongType()),\n",
    "                StructField(\"retweet_with_comment_timestamp\", LongType()),\n",
    "                StructField(\"like_timestamp\", LongType())       \n",
    "            ])\n",
    "        \n",
    "        self.trainFile = spark.read.csv(path=trainFile, sep=\"\\x01\", header=False, schema=SCHEMA)\n",
    "        \n",
    "        if MF:\n",
    "            self._preprocessMF()\n",
    "        else:\n",
    "            self._preprocess(trainsplit, seed)\n",
    "        #self.inputData = spark.createDataFrame(inputRDD, sep=\"\\x01\", schema=SCHEMA)    \n",
    "        \n",
    "        if testFile:\n",
    "            self.testFile = spark.read.csv(path=testFile, sep=\"\\x01\", header=False, schema=SCHEMA)\n",
    "            self._preprocessTest()\n",
    "    \n",
    "    '''\n",
    "        get the outputDF of the class, which is the result of the input after all preprocessing steps\n",
    "    '''\n",
    "    def getDF(self):\n",
    "        return self.processedTrainDF\n",
    "    \n",
    "    '''\n",
    "        get the preprocessed testDF\n",
    "    '''\n",
    "    def getTestDF(self):\n",
    "        return self.processedTestDF\n",
    "    \n",
    "    '''\n",
    "        return assembled DF, meaning all columsn from preprocessing steps are merged to one vector \n",
    "        (this is need for sparkML). This drops the labels\n",
    "    '''\n",
    "    def _assemble(self, df):\n",
    "        \n",
    "        cols = df.columns\n",
    "        cols.remove(\"like\") # remove labels and identifiers\n",
    "        cols.remove(\"retweet\")\n",
    "        cols.remove(\"reply\")\n",
    "        cols.remove(\"retweet_comment\")\n",
    "        cols.remove(\"tweet_id\")\n",
    "        cols.remove(\"engaging_user_id\")\n",
    "        assembler = VectorAssembler(inputCols=cols, outputCol=\"all_features\")\n",
    "        assembledDF = assembler.transform(df)\n",
    "        return assembledDF\n",
    "    \n",
    "    def _preprocessMF(self):\n",
    "        outputDF = self.trainFile\n",
    "        \n",
    "        self.outputDF = outputDF.select([\"tweet_id\",\"engaging_user_id\",\"engaged_with_user_id\",\n",
    "                                    \"retweet_timestamp\",\"reply_timestamp\",\n",
    "                                    \"retweet_with_comment_timestamp\",\"like_timestamp\"])\n",
    "    \n",
    "    def _preprocess(self, trainsplit, seed):\n",
    "        \n",
    "        outputDF = self.trainFile\n",
    "        \n",
    "        # Drop unnecessary cols\n",
    "        ### drop unused ids for classification\n",
    "        outputDF = outputDF.drop(\"engaged_with_user_id\").drop(\"engaged_user_id\")\\\n",
    "                    .drop(\"present_links\").drop(\"present_domains\")\n",
    "        #.drop(\"tweet_id\")\n",
    "        #.drop(\"engaging_user_id\")\n",
    "        \n",
    "        # Split the text tokens to valid format\n",
    "        textTokenizer = RegexTokenizer(inputCol=\"text_tokens\",outputCol=\"vector\", pattern=\"\\t\")\n",
    "        outputDF = textTokenizer.transform(outputDF)\n",
    "        hashtagTokenizer = RegexTokenizer(inputCol=\"hashtags\",outputCol=\"hashtag_tokens\", pattern=\"\\t\")\n",
    "        outputDF = hashtagTokenizer.transform(outputDF.fillna(\"none\", subset=[\"hashtags\"]))\n",
    "        \n",
    "        #self.tokenizerPipeline = Pipeline(stages=[textTokenizer, hashtagTokenizer])\n",
    "        #outputDF = self.tokenizerPipeline.fit(outputDF).transform(outputDF)\n",
    "        \n",
    "        outputDF = outputDF.drop(\"text_tokens\").withColumnRenamed(\"vector\", \"text_tokens\")\n",
    "        outputDF = outputDF.drop(\"hashtags\").withColumnRenamed(\"hashtag_tokens\", \"hashtags\")\n",
    "        \n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"present_media\", outputCol=\"media_list\")\n",
    "        outputDF = regexTokenizer.transform(outputDF.fillna(\"none\", subset=[\"present_media\"]))\n",
    "        outputDF = outputDF.drop(\"present_media\").withColumnRenamed(\"media_list\", \"present_media\")\n",
    "        outputDF = outputDF.withColumn(\"present_media2\", outputDF[\"present_media\"].cast(StringType()))\n",
    "        outputDF = outputDF.drop(\"present_media\").withColumnRenamed(\"present_media2\", \"present_media\")\n",
    "\n",
    "        # OneHotEncode tweet_type\n",
    "        ## TODO: user_id, engaged_user_id, ...\n",
    "        indexerTweetType = StringIndexer(inputCol=\"tweet_type\", outputCol=\"tweet_type_id\", handleInvalid=\"keep\" )\n",
    "        #outputDF = indexerTweetType.fit(outputDF).transform(outputDF)\n",
    "        indexerMedia = StringIndexer(inputCol=\"present_media\", outputCol=\"present_media_id\", handleInvalid=\"keep\")\n",
    "        #outputDF = indexerMedia.fit(outputDF).transform(outputDF)\n",
    "        indexerLang = StringIndexer(inputCol=\"language\", outputCol=\"language_id\", handleInvalid=\"keep\")\n",
    "        #outputDF = indexerLang.fit(outputDF).transform(outputDF)\n",
    "        \n",
    "        indexerPipeline = Pipeline(stages=[indexerTweetType, indexerMedia, indexerLang]) \n",
    "        self.indexerModel = indexerPipeline.fit(outputDF)\n",
    "        outputDF = self.indexerModel.transform(outputDF)\n",
    "        \n",
    "        # onehot\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[\"tweet_type_id\", \"present_media_id\", \"language_id\"],\n",
    "                                         outputCols=[\"tweet_type_onehot\", \"present_media_onehot\", \"language_onehot\"])\n",
    "        self.encoderModel = encoder.fit(outputDF)\n",
    "        outputDF = self.encoderModel.transform(outputDF)\n",
    "        \n",
    "        # for explainability safe this\n",
    "        self.explainOneHotDF = outputDF.select(\"tweet_type\", \"tweet_type_id\", \"tweet_type_onehot\",\n",
    "                                              \"present_media\", \"present_media_id\", \"present_media_onehot\",\n",
    "                                               \"language\", \"language_id\", \"language_onehot\"\n",
    "                                              )\n",
    "        # make label columns binary\n",
    "        outputDF = outputDF.withColumn(\"like\", when(outputDF[\"like_timestamp\"].isNull(), 0).otherwise(1))\n",
    "        outputDF = outputDF.withColumn(\"retweet\", when(outputDF[\"retweet_timestamp\"].isNull(), 0).otherwise(1))\n",
    "        outputDF = outputDF.withColumn(\"reply\", when(outputDF[\"reply_timestamp\"].isNull(), 0).otherwise(1))\n",
    "        outputDF = outputDF.withColumn(\"retweet_comment\", when(outputDF[\"retweet_with_comment_timestamp\"].isNull(), 0).otherwise(1))\n",
    "        \n",
    "        # drop intermediate columns\n",
    "        outputDF = outputDF.drop(*[\"like_timestamp\",\"retweet_timestamp\",\"reply_timestamp\",\n",
    "                                  \"retweet_with_comment_timestamp\",\"tweet_type\",\"tweet_type_id\",\n",
    "                                 \"language\",\"language_id\",\"present_media\",\"present_media_id\"])\n",
    "        \n",
    "        # tf/idf text + hashtags\n",
    "        ### hashtags\n",
    "        hashtagsTF = HashingTF(inputCol=\"hashtags\", outputCol=\"hashtagsTF\", numFeatures=2^10)\n",
    "        #outputDF = hashtagsTF.transform(outputDF)\n",
    "        hashtagsIDF = IDF(inputCol=\"hashtagsTF\", outputCol=\"hashtags_idf\")\n",
    "        #outputDF = self.hashtagsIDF.fit(outputDF).transform(outputDF)\n",
    "        \n",
    "        textTF = HashingTF(inputCol=\"text_tokens\", outputCol=\"tweet_text_TF\", numFeatures=2^14)\n",
    "        #outputDF = textTF.transform(outputDF)\n",
    "        textIDF = IDF(inputCol=\"tweet_text_TF\", outputCol=\"tweet_text_idf\")\n",
    "        #outputDF = self.textIDF.fit(outputDF).transform(outputDF)\n",
    "        \n",
    "        tfidfPipeline = Pipeline(stages=[hashtagsTF, hashtagsIDF, textTF, textIDF])\n",
    "        self.tfidfModel = tfidfPipeline.fit(outputDF)\n",
    "        outputDF = self.tfidfModel.transform(outputDF)\n",
    "        \n",
    "        outputDF = outputDF.drop(*[\"hashtags\", \"hashtagsTF\", \"text_tokens\", \"tweet_text_TF\"])\n",
    "        \n",
    "        # scaling\n",
    "        '''\n",
    "        scalerTimestamp = MinMaxScaler(inputCol=\"tweet_timestamp\",\n",
    "                                       outputCol=\"tweet_timestamp_scaled\")\n",
    "        scalerEngagedAccountCreation = MinMaxScaler(inputCol=\"engaged_with_user_account_creation\",\n",
    "                                                   outputCol=\"engaged_with_user_account_creation_scaled\")\n",
    "        scalerEngagingAccountCreation = MinMaxScaler(inputCol=\"engaging_user_account_creation\",\n",
    "                                                    outputCol=\"engaging_user_account_creation_scaled\")\n",
    "        \n",
    "        scalerEngagedFollowerCount = MinMaxScaler(inputCol=\"engaged_with_user_follower_count\",\n",
    "                                            outputCol=\"engaged_with_user_follower_count_scaled\")\n",
    "        scalerEngagedFollowingCount = MinMaxScaler(inputCol=\"engaged_with_user_following_count\",\n",
    "                                                  outputCol=\"engaged_with_user_following_count_scaled\")\n",
    "        scalerEngagingFollowerCount = MinMaxScaler(inputCol=\"engaging_user_follower_count\",\n",
    "                                           outputCol=\"engaging_user_follower_count_scaled\")\n",
    "        scalerEngagingFollowingCount = MinMaxScaler(inputCol=\"engaging_user_following_count\",\n",
    "                                                   outputCol=\"engaging_user_following_count_scaled\")\n",
    "        scalePipeline = Pipeline(stages=[scalerTimestamp, scalerEngagedAccountCreation,\n",
    "                                         scalerEngagingAccountCreation, scalerEngagedFollowerCount,\n",
    "                                        scalerEngagedFollowingCount, scalerEngagingFollowerCount,\n",
    "                                        scalerEngagingFollowingCount])\n",
    "        '''\n",
    "        ## first vectorize for spark... meh\n",
    "        assembler = VectorAssembler(inputCols=[\"tweet_timestamp\", \"engaged_with_user_account_creation\",\n",
    "                                   \"engaging_user_account_creation\", \"engaged_with_user_follower_count\",\n",
    "                                  \"engaged_with_user_following_count\", \"engaging_user_follower_count\",\n",
    "                                  \"engaging_user_following_count\"], outputCol=\"numeric_features\")\n",
    "        \n",
    "\n",
    "        numericScaler = MinMaxScaler(inputCol=\"numeric_features\", outputCol=\"numeric_scaled\")\n",
    "        scalePipeline = Pipeline(stages=[assembler, numericScaler])\n",
    "        self.scaleModel = scalePipeline.fit(outputDF)\n",
    "        outputDF = self.scaleModel.transform(outputDF)\n",
    "        \n",
    "        # drop numeric columns\n",
    "        outputDF = outputDF.drop(*[\"tweet_timestamp\", \"engaged_with_user_account_creation\",\n",
    "                                   \"engaging_user_account_creation\", \"engaged_with_user_follower_count\",\n",
    "                                  \"engaged_with_user_following_count\", \"engaging_user_follower_count\",\n",
    "                                  \"engaging_user_following_count\", \"numeric_features\"])\n",
    "        \n",
    "        outputDF = self._assemble(outputDF)\n",
    "        self.processedTrainDF = outputDF\n",
    "        \n",
    "        # might not need\n",
    "        # transform boolean to 0-1 column... first one has to change the type in the schema though \n",
    "        #data = data.select(\"engaging_user_is_verified\", \"engaged_with_user_is_verified\", \"engaged_follows_engaging\")\\\n",
    "        #    .replace([\"false\",\"true\"], [\"0\",\"1\"]).show()\n",
    "        \n",
    "        \n",
    "    '''\n",
    "        Preprocess test file if given...\n",
    "    '''\n",
    "    def _preprocessTest(self):\n",
    "        test = self.testFile\n",
    "        \n",
    "        ### repeat all the steps that went place for train\n",
    "        # Drop unnecessary cols\n",
    "        ### drop unused ids for classification\n",
    "        test = test.drop(\"engaged_with_user_id\").drop(\"engaged_user_id\")\\\n",
    "                    .drop(\"present_links\").drop(\"present_domains\")\n",
    "        \n",
    "        # Split the text tokens to valid format\n",
    "        textTokenizer = RegexTokenizer(inputCol=\"text_tokens\",outputCol=\"vector\", pattern=\"\\t\")\n",
    "        test = textTokenizer.transform(test)\n",
    "        hashtagTokenizer = RegexTokenizer(inputCol=\"hashtags\",outputCol=\"hashtag_tokens\", pattern=\"\\t\")\n",
    "        test = hashtagTokenizer.transform(test.fillna(\"none\", subset=[\"hashtags\"]))\n",
    "        \n",
    "        test = test.drop(\"text_tokens\").withColumnRenamed(\"vector\", \"text_tokens\")\n",
    "        test = test.drop(\"hashtags\").withColumnRenamed(\"hashtag_tokens\", \"hashtags\")\n",
    "        \n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"present_media\", outputCol=\"media_list\")\n",
    "        test = regexTokenizer.transform(test.fillna(\"none\", subset=[\"present_media\"]))\n",
    "        test = test.drop(\"present_media\").withColumnRenamed(\"media_list\", \"present_media\")\n",
    "        test = test.withColumn(\"present_media2\", test[\"present_media\"].cast(StringType()))\n",
    "        test = test.drop(\"present_media\").withColumnRenamed(\"present_media2\", \"present_media\")\n",
    "        \n",
    "        ### REUSE MODELS FROM PROCESSING TRAIN\n",
    "        test = self.indexerModel.transform(test)\n",
    "        test = self.encoderModel.transform(test)\n",
    "        test = test.drop(*[\"tweet_type\",\"tweet_type_id\", \"language\",\"language_id\",\"present_media\",\"present_media_id\"])\n",
    "        # tf/idf text + hashtags\n",
    "        test = self.tfidfModel.transform(test)\n",
    "        test = test.drop(*[\"hashtags\", \"hashtagsTF\", \"text_tokens\", \"tweet_text_TF\"])\n",
    "\n",
    "        # scale numeric\n",
    "        test = self.scaleModel.transform(test)\n",
    "        \n",
    "        # drop numeric columns\n",
    "        test = test.drop(*[\"tweet_timestamp\", \"engaged_with_user_account_creation\",\n",
    "                                   \"engaging_user_account_creation\", \"engaged_with_user_follower_count\",\n",
    "                                  \"engaged_with_user_following_count\", \"engaging_user_follower_count\",\n",
    "                                  \"engaging_user_following_count\", \"numeric_features\"])\n",
    "        \n",
    "        # rename target columns\n",
    "        test = test.withColumnRenamed(\"like_timestamp\", \"like\")#.drop(\"like_timestamp\")\n",
    "        test = test.withColumnRenamed(\"retweet_timestamp\", \"retweet\")\n",
    "        test = test.withColumnRenamed(\"reply_timestamp\", \"reply\")\n",
    "        test = test.withColumnRenamed(\"retweet_with_comment_timestamp\", \"retweet_comment\")\n",
    "        test = self._assemble(test)\n",
    "        \n",
    "        #outputDF = outputDF.withColumn(\"like\", when(outputDF[\"like_timestamp\"].isNull(), 0).otherwise(1))\n",
    "        #outputDF = outputDF.withColumn(\"retweet\", when(outputDF[\"retweet_timestamp\"].isNull(), 0).otherwise(1))\n",
    "        #outputDF = outputDF.withColumn(\"reply\", when(outputDF[\"reply_timestamp\"].isNull(), 0).otherwise(1))\n",
    "        #outputDF = outputDF.withColumn(\"retweet_comment\", when(outputDF[\"retweet_with_comment_timestamp\"].isNull(), 0).otherwise(1))\n",
    "\n",
    "        \n",
    "        self.processedTestDF = test\n",
    "    \n",
    "    '''\n",
    "        returns small dataframe that explains the values of the oneHotEncoder step, this might be needed\n",
    "        for mapping the encodings back to the original values\n",
    "    '''\n",
    "    def explainOneHot(self):\n",
    "        return self.explainOneHotDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = \"///user/e11920598/traintweet_1000.tsv\"\n",
    "train = \"///tmp/traintweet_1000.tsv\"\n",
    "testfile = \"///user/e11920598/test_1000.tsv\"\n",
    "# the full train file has 121.386.431 lines\n",
    "#train = \"///user/pknees/RSC20/training.tsv\"\n",
    "# the full test file has 12.434.838 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_preproc\n",
    "import importlib\n",
    "importlib.reload(twitter_preproc)\n",
    "from twitter_preproc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    def __init__(self, spark:SparkSession, sc:SparkContext, inputFile:str, seed:int=123,\n",
    "#                 MF:bool=False, trainsplit:float=0.9):\n",
    "preproc = twitter_preproc(spark, sc, train, testFile=testfile)\n",
    "\n",
    "traindata = preproc.getDF()\n",
    "testdata = preproc.getTestDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "preds = [\"like\", \"retweet\", \"reply\", \"retweet_comment\"]\n",
    "\n",
    "for feat in preds:\n",
    "    rf = RandomForestClassifier(labelCol=feat, featuresCol=\"all_features\", numTrees=20, maxDepth=5, seed=42)\n",
    "    model = rf.fit(traindata)\n",
    "    pred = model.transform(testdata)\n",
    "    pred_out = pred.select(\"tweet_id\",\"engaging_user_id\",\"probability\",\"prediction\")\n",
    "    pd.DataFrame(pred_out.collect(),\n",
    "                 columns=[\"tweet_id\",\"engaging_user_id\",\"probability\",\"prediction\"])\\\n",
    "        .to_csv(\"../output/\" + feat + \"_out.csv\", sep=\",\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file rforest.py\n",
    "\n",
    "import twitter_preproc\n",
    "import importlib\n",
    "importlib.reload(twitter_preproc)\n",
    "from twitter_preproc import *\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from twitter_preproc import twitter_preproc\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"ChiSquareSpark\").getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"dic-recsys\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "train = \"///tmp/traintweet_1000.tsv\"\n",
    "testfile = \"///user/e11920598/test_1000.tsv\"\n",
    "# the full train file has 121.386.431 lines\n",
    "#train = \"///user/pknees/RSC20/training.tsv\"\n",
    "# the full test file has 12.434.838 lines\n",
    "#train = \"///user/pknees/RSC20/test.tsv\"\n",
    "\n",
    "preproc = twitter_preproc(spark, sc, train, testFile=testfile)\n",
    "\n",
    "traindata = preproc.getDF()\n",
    "testdata = preproc.getTestDF()\n",
    "\n",
    "preds = [\"like\", \"retweet\", \"reply\", \"retweet_comment\"]\n",
    "\n",
    "for feat in preds:\n",
    "    # train a random forest with default vals...\n",
    "    rf = RandomForestClassifier(labelCol=feat, featuresCol=\"all_features\", numTrees=20, maxDepth=5, seed=42)\n",
    "    model = rf.fit(traindata)\n",
    "    pred = model.transform(testdata)\n",
    "    pred_out = pred.select(\"tweet_id\",\"engaging_user_id\",\"probability\",\"prediction\")\n",
    "    pd.DataFrame(pred_out.collect(),\n",
    "                 columns=[\"tweet_id\",\"engaging_user_id\",\"probability\",\"prediction\"])\\\n",
    "        .to_csv(\"../output/\" + feat + \"_out.csv\", sep=\",\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5710180623973728\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>like</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.31996544825569323, 0.6800345517443068]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.46575528395631166, 0.5342447160436883]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.5709776218800888, 0.4290223781199112]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7146769647972976, 0.28532303520270247]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.42696299334607807, 0.5730370066539219]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7337220883356758, 0.2662779116643242]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.5293333560655562, 0.47066664393444385]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7790026583406632, 0.22099734165933677]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.8451416127191497, 0.15485838728085033]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.4149282748786473, 0.5850717251213526]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.6941760934670735, 0.3058239065329265]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.4646797644507924, 0.5353202355492076]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7158382525552611, 0.2841617474447388]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.3646027507178475, 0.6353972492821526]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.4652977160578004, 0.5347022839421995]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.5447545743596729, 0.4552454256403271]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.7316725725213113, 0.26832742747868865]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.36937943133152185, 0.6306205686684782]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.33195236093288855, 0.6680476390671115]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.3490070014317879, 0.6509929985682121]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.6192249603002834, 0.3807750396997166]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.43302388384552193, 0.5669761161544782]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.7127085953272433, 0.2872914046727567]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.3779427022181291, 0.6220572977818709]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7590909574481699, 0.24090904255183004]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.5979174173097772, 0.4020825826902227]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.4724256898521255, 0.5275743101478745]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.6666071653538369, 0.33339283464616304]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7354550198989168, 0.2645449801010833]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.4238067426847879, 0.5761932573152122]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.5278070311062303, 0.47219296889376966]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.5098306063733946, 0.4901693936266054]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.46033277437797004, 0.53966722562203]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.6221915150539423, 0.3778084849460576]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.4902197990587725, 0.5097802009412274]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.4259160474168747, 0.5740839525831253]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.3937713358380247, 0.6062286641619752]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.5845151648194113, 0.4154848351805887]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.7640684121208114, 0.2359315878791886]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.41901792075345473, 0.5809820792465452]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.6772241054084357, 0.32277589459156425]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.415805244054554, 0.5841947559454459]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.7211890433575727, 0.27881095664242733]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.5052662132033353, 0.4947337867966647]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.545039650220948, 0.4549603497790519]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.3514642857100069, 0.6485357142899931]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.701611744352681, 0.2983882556473189]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.46215804214669165, 0.5378419578533084]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.4828607148571645, 0.5171392851428356]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.8128848947212679, 0.18711510527873199]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.4270577553987258, 0.5729422446012742]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7937867182066618, 0.20621328179333825]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.41187851092977834, 0.5881214890702218]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.6633111490674183, 0.3366888509325818]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.4681400311291045, 0.5318599688708954]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.7357416142013515, 0.26425838579864835]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.4027699652625666, 0.5972300347374334]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.4116475244969765, 0.5883524755030235]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>[0.5166989629849441, 0.4833010370150559]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>[0.7262323141692874, 0.27376768583071265]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    like                                probability  prediction\n",
       "0      1  [0.31996544825569323, 0.6800345517443068]         1.0\n",
       "1      0  [0.46575528395631166, 0.5342447160436883]         1.0\n",
       "2      0   [0.5709776218800888, 0.4290223781199112]         0.0\n",
       "3      0  [0.7146769647972976, 0.28532303520270247]         0.0\n",
       "4      1  [0.42696299334607807, 0.5730370066539219]         1.0\n",
       "5      0   [0.7337220883356758, 0.2662779116643242]         0.0\n",
       "6      1  [0.5293333560655562, 0.47066664393444385]         0.0\n",
       "7      0  [0.7790026583406632, 0.22099734165933677]         0.0\n",
       "8      1  [0.8451416127191497, 0.15485838728085033]         0.0\n",
       "9      0   [0.4149282748786473, 0.5850717251213526]         1.0\n",
       "10     1   [0.6941760934670735, 0.3058239065329265]         0.0\n",
       "11     1   [0.4646797644507924, 0.5353202355492076]         1.0\n",
       "12     0   [0.7158382525552611, 0.2841617474447388]         0.0\n",
       "13     0   [0.3646027507178475, 0.6353972492821526]         1.0\n",
       "14     1   [0.4652977160578004, 0.5347022839421995]         1.0\n",
       "15     0   [0.5447545743596729, 0.4552454256403271]         0.0\n",
       "16     1  [0.7316725725213113, 0.26832742747868865]         0.0\n",
       "17     0  [0.36937943133152185, 0.6306205686684782]         1.0\n",
       "18     1  [0.33195236093288855, 0.6680476390671115]         1.0\n",
       "19     1   [0.3490070014317879, 0.6509929985682121]         1.0\n",
       "20     0   [0.6192249603002834, 0.3807750396997166]         0.0\n",
       "21     0  [0.43302388384552193, 0.5669761161544782]         1.0\n",
       "22     1   [0.7127085953272433, 0.2872914046727567]         0.0\n",
       "23     1   [0.3779427022181291, 0.6220572977818709]         1.0\n",
       "24     0  [0.7590909574481699, 0.24090904255183004]         0.0\n",
       "25     0   [0.5979174173097772, 0.4020825826902227]         0.0\n",
       "26     0   [0.4724256898521255, 0.5275743101478745]         1.0\n",
       "27     0  [0.6666071653538369, 0.33339283464616304]         0.0\n",
       "28     0   [0.7354550198989168, 0.2645449801010833]         0.0\n",
       "29     1   [0.4238067426847879, 0.5761932573152122]         1.0\n",
       "..   ...                                        ...         ...\n",
       "70     0  [0.5278070311062303, 0.47219296889376966]         0.0\n",
       "71     0   [0.5098306063733946, 0.4901693936266054]         0.0\n",
       "72     1    [0.46033277437797004, 0.53966722562203]         1.0\n",
       "73     1   [0.6221915150539423, 0.3778084849460576]         0.0\n",
       "74     0   [0.4902197990587725, 0.5097802009412274]         1.0\n",
       "75     0   [0.4259160474168747, 0.5740839525831253]         1.0\n",
       "76     0   [0.3937713358380247, 0.6062286641619752]         1.0\n",
       "77     0   [0.5845151648194113, 0.4154848351805887]         0.0\n",
       "78     1   [0.7640684121208114, 0.2359315878791886]         0.0\n",
       "79     0  [0.41901792075345473, 0.5809820792465452]         1.0\n",
       "80     0  [0.6772241054084357, 0.32277589459156425]         0.0\n",
       "81     1    [0.415805244054554, 0.5841947559454459]         1.0\n",
       "82     1  [0.7211890433575727, 0.27881095664242733]         0.0\n",
       "83     1   [0.5052662132033353, 0.4947337867966647]         0.0\n",
       "84     0    [0.545039650220948, 0.4549603497790519]         0.0\n",
       "85     1   [0.3514642857100069, 0.6485357142899931]         1.0\n",
       "86     1    [0.701611744352681, 0.2983882556473189]         0.0\n",
       "87     1  [0.46215804214669165, 0.5378419578533084]         1.0\n",
       "88     0   [0.4828607148571645, 0.5171392851428356]         1.0\n",
       "89     0  [0.8128848947212679, 0.18711510527873199]         0.0\n",
       "90     0   [0.4270577553987258, 0.5729422446012742]         1.0\n",
       "91     0  [0.7937867182066618, 0.20621328179333825]         0.0\n",
       "92     1  [0.41187851092977834, 0.5881214890702218]         1.0\n",
       "93     1   [0.6633111490674183, 0.3366888509325818]         0.0\n",
       "94     1   [0.4681400311291045, 0.5318599688708954]         1.0\n",
       "95     0  [0.7357416142013515, 0.26425838579864835]         0.0\n",
       "96     1   [0.4027699652625666, 0.5972300347374334]         1.0\n",
       "97     0   [0.4116475244969765, 0.5883524755030235]         1.0\n",
       "98     0   [0.5166989629849441, 0.4833010370150559]         0.0\n",
       "99     1  [0.7262323141692874, 0.27376768583071265]         0.0\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "train, test = traindata.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "#grid = ParamGridBuilder().addGrid(rflike.maxDepth,[5])\n",
    "#        .addGrid(rflike.seed,[42])\n",
    "#        .addGrid(rflike.numTrees=[50,100,200,])\n",
    "#        .addGrid(rflike.num)\n",
    "#        .build()\n",
    "rflike = RandomForestClassifier(labelCol=\"like\", featuresCol=\"all_features\", numTrees=20, maxDepth=5, seed=42)\n",
    "model = rflike.fit(train)\n",
    "\n",
    "pred = model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"like\")\n",
    "print(evaluator.evaluate(pred))\n",
    "\n",
    "#pred.show()\n",
    "pd.DataFrame(pred.select([\"like\",\"probability\",\"prediction\"]).collect(), columns=[\"like\", \"probability\", \"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------------------+------------------------+-----------------+--------------------+---------------+----+-------+-----+---------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|engaged_with_user_is_verified|engaging_user_is_verified|engaged_follows_engaging|tweet_type_onehot|present_media_onehot|language_onehot|like|retweet|reply|retweet_comment|      numeric_scaled|        hashtags_idf|      tweet_text_idf|        all_features|\n",
      "+-----------------------------+-------------------------+------------------------+-----------------+--------------------+---------------+----+-------+-----+---------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[1],[1.0])|   0|      0|    0|              0|[0.52188082629834...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(68,[3,5,12,41,42...|\n",
      "|                        false|                    false|                    true|    (2,[1],[1.0])|       (6,[0],[1.0])| (30,[1],[1.0])|   1|      1|    0|              0|[0.90990092294355...|(8,[1,6,7],[3.147...|(12,[0,1,2,3,4,5,...|(68,[2,4,5,12,41,...|\n",
      "|                        false|                    false|                    true|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[1],[1.0])|   1|      0|    0|              0|[0.05178509096263...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(68,[2,3,5,12,41,...|\n",
      "|                        false|                    false|                   false|    (2,[1],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|   1|      0|    0|              0|[0.61974692255644...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(68,[4,5,11,41,42...|\n",
      "|                         true|                    false|                   false|    (2,[0],[1.0])|       (6,[1],[1.0])| (30,[1],[1.0])|   1|      0|    0|              0|[0.01581858095980...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(68,[0,3,6,12,41,...|\n",
      "+-----------------------------+-------------------------+------------------------+-----------------+--------------------+---------------+----+-------+-----+---------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+---+\n",
      "|like| id|\n",
      "+----+---+\n",
      "|   0|  0|\n",
      "|   1|  1|\n",
      "|   1|  2|\n",
      "|   1|  3|\n",
      "|   1|  4|\n",
      "+----+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+---+\n",
      "|        all_features| id|\n",
      "+--------------------+---+\n",
      "|(68,[3,5,12,41,42...|  0|\n",
      "|(68,[2,4,5,12,41,...|  1|\n",
      "|(68,[2,3,5,12,41,...|  2|\n",
      "|(68,[4,5,11,41,42...|  3|\n",
      "|(68,[0,3,6,12,41,...|  4|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+----+--------------------+\n",
      "| id|like|        all_features|\n",
      "+---+----+--------------------+\n",
      "|  0|   0|(68,[3,5,12,41,42...|\n",
      "|  1|   1|(68,[2,4,5,12,41,...|\n",
      "|  2|   1|(68,[2,3,5,12,41,...|\n",
      "|  3|   1|(68,[4,5,11,41,42...|\n",
      "|  4|   1|(68,[0,3,6,12,41,...|\n",
      "+---+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assd = preproc.getAssembledDF()\n",
    "assd.show(5)\n",
    "like = assd.select(\"like\").withColumn(\"id\", monotonically_increasing_id())\n",
    "feat = assd.select(\"all_features\").withColumn(\"id\", monotonically_increasing_id())\n",
    "like.show(5)\n",
    "feat.show(5)\n",
    "foo = like.join(feat, \"id\")\n",
    "foo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-------------------------+------------------------+-----------------+--------------------+---------------+-------+-----+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+\n",
      "|engaged_with_user_is_verified|engaging_user_is_verified|engaged_follows_engaging|tweet_type_onehot|present_media_onehot|language_onehot|retweet|reply|retweet_comment|      numeric_scaled|        hashtags_idf|      tweet_text_idf|        all_features|       rawPrediction|         probability|prediction|truth|\n",
      "+-----------------------------+-------------------------+------------------------+-----------------+--------------------+---------------+-------+-----+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+\n",
      "|                        false|                    false|                   false|        (2,[],[])|       (6,[0],[1.0])| (30,[0],[1.0])|      0|    1|              0|[0.45977279607728...|(8,[7],[0.1646955...|(12,[2,7,8,11],[0...|(72,[5,11,43,45,4...|[8.96293372580501...|[0.89629337258050...|       0.0|    0|\n",
      "|                        false|                    false|                   false|        (2,[],[])|       (6,[0],[1.0])| (30,[5],[1.0])|      1|    0|              1|[0.67403664974300...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[5,16,42,44,4...|[5.82091478378607...|[0.58209147837860...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|           (6,[],[])| (30,[0],[1.0])|      0|    0|              0|[0.36076687527606...|(8,[7],[0.1646955...|(12,[0,1,2,4,5,6,...|(72,[3,11,41,45,4...|[2.48244257657564...|[0.24824425765756...|       1.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|           (6,[],[])| (30,[9],[1.0])|      0|    0|              0|[0.77963998749336...|(8,[2,4,7],[3.245...|(12,[0,1,2,3,4,5,...|(72,[3,20,45,46,4...|[8.34401709401709...|[0.83440170940170...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|      0|    0|              0|[0.18317592505281...|(8,[0,2,4,6],[2.8...|(12,[0,1,2,3,4,5,...|(72,[3,5,11,45,46...|[7.71691420970959...|[0.77169142097095...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|      0|    0|              0|[0.26434665224102...|(8,[2],[3.2451931...|(12,[0,1,2,3,4,5,...|(72,[3,5,11,45,46...|[8.71691420970959...|[0.87169142097095...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|      0|    0|              0|[0.35599417018622...|(8,[7],[0.1646955...|(12,[0,1,2,4,5,6,...|(72,[3,5,11,45,46...|[7.70022035435342...|[0.77002203543534...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|      0|    0|              0|[0.39542647469970...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,11,45,46...|[7.98661491217655...|[0.79866149121765...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|      0|    0|              0|[0.44858134029470...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,11,45,46...|[7.89082725318785...|[0.78908272531878...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|      0|    0|              0|[0.68074656026098...|(8,[7],[0.1646955...|(12,[1,2,3,4,5,6,...|(72,[3,5,11,45,46...|[6.89405080961245...|[0.68940508096124...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|      0|    1|              0|[4.9960379103327E...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,11,43,45...|[9.13636736913362...|[0.91363673691336...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[0],[1.0])|      1|    0|              0|[0.73419754863278...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,11,41,42...|[3.21129261542568...|[0.32112926154256...|       1.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[1],[1.0])|      0|    0|              0|[0.08120050489760...|(8,[7],[0.1646955...|(12,[1,2,6,7,8,9]...|(72,[3,5,12,45,46...|[7.9921897382746,...|[0.79921897382746...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[1],[1.0])|      0|    0|              0|[0.52188082629834...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,12,45,46...|[7.59355368768676...|[0.75935536876867...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[1],[1.0])|      0|    0|              0|[0.12146162232269...|(8,[7],[0.1646955...|(12,[0,1,2,4,5,6,...|(72,[3,5,12,41,45...|[1.33355368768676...|[0.13335536876867...|       1.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[1],[1.0])|      0|    0|              0|[0.35481795066495...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,12,41,45...|[1.92169471332778...|[0.19216947133277...|       1.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[1],[1.0])|      0|    0|              0|[0.58598561736636...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,12,41,45...|[2.41829335278356...|[0.24182933527835...|       1.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[3],[1.0])|      0|    0|              0|[0.01010126075512...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,14,45,46...|[7.98098958512265...|[0.79809895851226...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[3],[1.0])|      0|    0|              0|[0.64522340726476...|(8,[7],[0.1646955...|(12,[0,1,2,3,4,5,...|(72,[3,5,14,45,46...|[8.41829335278356...|[0.84182933527835...|       0.0|    0|\n",
      "|                        false|                    false|                   false|    (2,[0],[1.0])|       (6,[0],[1.0])| (30,[3],[1.0])|      0|    0|              0|[0.04059859813161...|(8,[7],[0.1646955...|(12,[0,3,4,5,6,7,...|(72,[3,5,14,41,45...|[1.93323282217482...|[0.19332328221748...|       1.0|    0|\n",
      "+-----------------------------+-------------------------+------------------------+-----------------+--------------------+---------------+-------+-----+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+\n",
      "|like|\n",
      "+----+\n",
      "|   0|\n",
      "|   0|\n",
      "|   1|\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "|   0|\n",
      "|   1|\n",
      "|   0|\n",
      "|   0|\n",
      "|   1|\n",
      "|   1|\n",
      "|   1|\n",
      "|   0|\n",
      "|   0|\n",
      "|   1|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 0, placement implies 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   4242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4243\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4244\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'y'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-dff45ba5b079>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mframe_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3195\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   4244\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4245\u001b[0m             \u001b[0;31m# This item wasn't present, just insert at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4246\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4247\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4346\u001b[0m         block = make_block(values=value, ndim=self.ndim,\n\u001b[0;32m-> 4347\u001b[0;31m                            placement=slice(loc, loc + 1))\n\u001b[0m\u001b[1;32m   4348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m \u001b[0;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    123\u001b[0m             raise ValueError(\n\u001b[1;32m    124\u001b[0m                 \u001b[0;34m'Wrong number of items passed {val}, placement implies '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 '{mgr}'.format(val=len(self.values), mgr=len(self.mgr_locs)))\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_ndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 0, placement implies 1"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "data = preproc.getDF()\n",
    "data.drop(*[\"retweet\",\"reply\",\"retweet_comment\"])\n",
    "cols = data.columns\n",
    "assembler = VectorAssembler(inputCols=cols, outputCol=\"all_features\")\n",
    "data = assembler.transform(data)\n",
    "train, test = data.randomSplit([0.8,0.2])\n",
    "rf = RandomForestClassifier(labelCol=\"like\", featuresCol=\"all_features\", numTrees=10)\n",
    "model = rf.fit(train)\n",
    "\n",
    "test_y = test.select(\"like\")\n",
    "test = test.drop(\"like\")\n",
    "pred = model.transform(test)\n",
    "pred = pred.withColumn(\"truth\", lit(0))\n",
    "#pred.show()\n",
    "frame = pd.DataFrame(pred.show(), columns=pred.columns)\n",
    "frame_y = pd.DataFrame(test_y.show())\n",
    "frame[\"y\"] = frame_y\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|like_timestamp|\n",
      "+--------------+\n",
      "|          null|\n",
      "|    1581497622|\n",
      "|    1581060554|\n",
      "|    1581328518|\n",
      "|    1580957807|\n",
      "|    1581346588|\n",
      "|          null|\n",
      "|          null|\n",
      "|    1581009248|\n",
      "|          null|\n",
      "|    1581189873|\n",
      "|          null|\n",
      "|    1581045318|\n",
      "|    1581375276|\n",
      "|    1581063697|\n",
      "|          null|\n",
      "|    1581017998|\n",
      "|          null|\n",
      "|          null|\n",
      "|    1581260483|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------+----+\n",
      "|like_timestamp|like|\n",
      "+--------------+----+\n",
      "|          null|   0|\n",
      "|    1581497622|   1|\n",
      "|    1581060554|   1|\n",
      "|    1581328518|   1|\n",
      "|    1580957807|   1|\n",
      "|    1581346588|   1|\n",
      "|          null|   0|\n",
      "|          null|   0|\n",
      "|    1581009248|   1|\n",
      "|          null|   0|\n",
      "|    1581189873|   1|\n",
      "|          null|   0|\n",
      "|    1581045318|   1|\n",
      "|    1581375276|   1|\n",
      "|    1581063697|   1|\n",
      "|          null|   0|\n",
      "|    1581017998|   1|\n",
      "|          null|   0|\n",
      "|          null|   0|\n",
      "|    1581260483|   1|\n",
      "+--------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "### Tweet-Type OneHotEncodings:\n",
      "+----------+-------------+-----------------+--------------------+----------------+--------------------+--------------------+-----------+---------------+\n",
      "|tweet_type|tweet_type_id|tweet_type_onehot|       present_media|present_media_id|present_media_onehot|            language|language_id|language_onehot|\n",
      "+----------+-------------+-----------------+--------------------+----------------+--------------------+--------------------+-----------+---------------+\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|              [none]|             0.0|       (6,[0],[1.0])|22C448FF81263D4BA...|        1.0| (30,[1],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|              [none]|             0.0|       (6,[0],[1.0])|22C448FF81263D4BA...|        1.0| (30,[1],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|              [none]|             0.0|       (6,[0],[1.0])|22C448FF81263D4BA...|        1.0| (30,[1],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|              [none]|             0.0|       (6,[0],[1.0])|D3164C7FBCF2565DD...|        0.0| (30,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|             [photo]|             1.0|       (6,[1],[1.0])|22C448FF81263D4BA...|        1.0| (30,[1],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|             [video]|             2.0|       (6,[2],[1.0])|167115458A0DBDFF7...|        5.0| (30,[5],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|[photo, photo, ph...|             5.0|       (6,[5],[1.0])|ECED8A16BE2A5E887...|        3.0| (30,[3],[1.0])|\n",
      "|     Quote|          2.0|        (2,[],[])|              [none]|             0.0|       (6,[0],[1.0])|ECED8A16BE2A5E887...|        3.0| (30,[3],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|             [video]|             2.0|       (6,[2],[1.0])|D3164C7FBCF2565DD...|        0.0| (30,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|             [video]|             2.0|       (6,[2],[1.0])|D3164C7FBCF2565DD...|        0.0| (30,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|             [photo]|             1.0|       (6,[1],[1.0])|4DC22C3F31C5C4372...|        6.0| (30,[6],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|      [photo, photo]|             3.0|       (6,[3],[1.0])|D3164C7FBCF2565DD...|        0.0| (30,[0],[1.0])|\n",
      "|     Quote|          2.0|        (2,[],[])|              [none]|             0.0|       (6,[0],[1.0])|D3164C7FBCF2565DD...|        0.0| (30,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|              [none]|             0.0|       (6,[0],[1.0])|D3164C7FBCF2565DD...|        0.0| (30,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|[photo, photo, ph...|             5.0|       (6,[5],[1.0])|22C448FF81263D4BA...|        1.0| (30,[1],[1.0])|\n",
      "|     Quote|          2.0|        (2,[],[])|              [none]|             0.0|       (6,[0],[1.0])|ECED8A16BE2A5E887...|        3.0| (30,[3],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|              [none]|             0.0|       (6,[0],[1.0])|D3164C7FBCF2565DD...|        0.0| (30,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|      [photo, photo]|             3.0|       (6,[3],[1.0])|D3164C7FBCF2565DD...|        0.0| (30,[0],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|             [video]|             2.0|       (6,[2],[1.0])|06D61DCBBE938971E...|        2.0| (30,[2],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|             [video]|             2.0|       (6,[2],[1.0])|06D61DCBBE938971E...|        2.0| (30,[2],[1.0])|\n",
      "+----------+-------------+-----------------+--------------------+----------------+--------------------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import * \n",
    "\n",
    "data = preproc.getDF()\n",
    "data.select(\"like_timestamp\").show()\n",
    "foo = data.withColumn(\"like\", when(data[\"like_timestamp\"].isNull(), 0).otherwise(1))\n",
    "foo.select(\"like_timestamp\", \"like\").show()\n",
    "#data = data.drop(\"text_tokens\").withColumnRenamed(\"vector\", \"text_tokens\")\n",
    "print(\"### Tweet-Type OneHotEncodings:\")\n",
    "explainonehot = preproc.explainOneHot()\n",
    "explainonehot.show()\n",
    "#data.show()\n",
    "#data.groupBy(\"engaging_user_is_verified\").count().show()\n",
    "#data = data.select(\"engaging_user_is_verified\", \"engaged_with_user_is_verified\", \"engaged_follows_engaging\")\\\n",
    "#    .replace([\"false\",\"true\"], [\"0\",\"1\"])..show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|present_media|\n",
      "+-------------+\n",
      "|       [none]|\n",
      "|       [none]|\n",
      "|       [none]|\n",
      "|       [none]|\n",
      "|      [photo]|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[hashtags: string, tweet_type: string, language: string, tweet_timestamp: bigint, engaged_with_user_follower_count: bigint, engaged_with_user_following_count: bigint, engaged_with_user_is_verified: boolean, engaged_with_user_account_creation: bigint, engaging_user_id: string, engaging_user_follower_count: bigint, engaging_user_following_count: bigint, engaging_user_is_verified: boolean, engaging_user_account_creation: bigint, engaged_follows_engaging: boolean, reply_timestamp: bigint, retweet_timestamp: bigint, retweet_with_comment_timestamp: bigint, like_timestamp: bigint, text_tokens: array<string>, present_media: string, tweet_type_id: double, present_media_id: double, tweet_type_onehot: vector, present_media_onehot: vector, present_media2: string]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(\"present_media\").show(5)\n",
    "data.withColumn(\"present_media2\", data[\"present_media\"].cast(StringType()))\n",
    "#data.select(\"present_media\").rdd.map(lambda x: str(x[0])).toDF(schema= StructType([\n",
    "#                StructField(\"present_media\", StringType())])).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PythonRDD[63] at RDD at PythonRDD.scala:53'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.foreach(print)\n",
    "str(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                                 outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "data = preproc.getDF()\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text_tokens\",outputCol=\"vector\", pattern=\"\\t\")\n",
    "tokenized = regexTokenizer.transform(data)\n",
    "\n",
    "tokenized.select(\"vector\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo.py\n"
     ]
    }
   ],
   "source": [
    "%%file demo.py\n",
    "\n",
    "\n",
    "from twitter_preproc import twitter_preproc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"ChiSquareSpark\").getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# sample file with 1000 tweets for checking the pipeline\n",
    "train = \"///user/e11920598/traintweet_1000.tsv\"\n",
    "\n",
    "preproc = twitter_preproc(spark, sc, train)\n",
    "print(preproc.getDF().show(5))\n",
    "\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "20/06/06 16:26:47 INFO spark.SparkContext: Running Spark version 2.4.0-cdh6.3.2\n",
      "20/06/06 16:26:47 INFO logging.DriverLogger: Added a local log appender at: /tmp/spark-95acd95f-2bb4-4950-bc77-662ca74baeab/__driver_logs__/driver.log\n",
      "20/06/06 16:26:47 INFO spark.SparkContext: Submitted application: Pipeline\n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: Changing view acls to: e11920598\n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: Changing modify acls to: e11920598\n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(e11920598); groups with view permissions: Set(); users  with modify permissions: Set(e11920598); groups with modify permissions: Set()\n",
      "20/06/06 16:26:47 INFO util.Utils: Successfully started service 'sparkDriver' on port 40223.\n",
      "20/06/06 16:26:47 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "20/06/06 16:26:47 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "20/06/06 16:26:47 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/06/06 16:26:47 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/06/06 16:26:47 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b5e841a6-5826-4e85-9c72-4c4474dab04a\n",
      "20/06/06 16:26:47 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/06/06 16:26:47 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "20/06/06 16:26:47 INFO util.log: Logging initialized @2856ms\n",
      "20/06/06 16:26:47 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-09-04T23:11:46+02:00, git hash: 3ce520221d0240229c862b122d2b06c12a625732\n",
      "20/06/06 16:26:47 INFO server.Server: Started @2968ms\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "20/06/06 16:26:47 INFO server.AbstractConnector: Started ServerConnector@6b588ecd{HTTP/1.1,[http/1.1]}{0.0.0.0:4050}\n",
      "20/06/06 16:26:47 INFO util.Utils: Successfully started service 'SparkUI' on port 4050.\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5aad2a0b{/jobs,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b56f2cf{/jobs/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b231bc7{/jobs/job,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7dbdd439{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51d191cb{/stages,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27f5515{/stages/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62fad302{/stages/stage,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54d87832{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@608ed368{/stages/pool,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f5fde0e{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2904f346{/storage,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e8a07bc{/storage/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28899696{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69678227{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67f7c28e{/environment,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9ae8e2b{/environment/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d7794ae{/executors,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38d4459d{/executors/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@616e13bb{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ccc166b{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d4f23b8{/static,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@787dc45e{/,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2beb43cd{/api,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a626254{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c395428{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://c100.local:4050\n",
      "20/06/06 16:26:48 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:48 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/06/06 16:26:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/06/06 16:26:48 INFO client.RMProxy: Connecting to ResourceManager at c100.local/10.7.0.100:8032\n",
      "20/06/06 16:26:49 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:49 INFO yarn.Client: Requesting a new application from cluster with 18 NodeManagers\n",
      "20/06/06 16:26:49 INFO conf.Configuration: resource-types.xml not found\n",
      "20/06/06 16:26:49 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "20/06/06 16:26:49 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (143360 MB per container)\n",
      "20/06/06 16:26:49 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "20/06/06 16:26:49 INFO yarn.Client: Setting up container launch context for our AM\n",
      "20/06/06 16:26:49 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "20/06/06 16:26:49 INFO yarn.Client: Preparing resources for our AM container\n",
      "20/06/06 16:26:50 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:51 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:51 INFO yarn.Client: Uploading resource file:/tmp/spark-95acd95f-2bb4-4950-bc77-662ca74baeab/__spark_conf__4692943125962393701.zip -> hdfs://nameservice1/user/e11920598/.sparkStaging/application_1591257126602_0273/__spark_conf__.zip\n",
      "20/06/06 16:26:52 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:53 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:54 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:55 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:58 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:59 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:00 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: Changing view acls to: e11920598\n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: Changing modify acls to: e11920598\n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(e11920598); groups with view permissions: Set(); users  with modify permissions: Set(e11920598); groups with modify permissions: Set()\n",
      "20/06/06 16:27:00 INFO yarn.Client: Submitting application application_1591257126602_0273 to ResourceManager\n",
      "20/06/06 16:27:00 INFO impl.YarnClientImpl: Submitted application application_1591257126602_0273\n",
      "20/06/06 16:27:01 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:01 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:01 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Sat Jun 06 16:27:01 +0200 2020] Application is added to the scheduler and is not yet activated.  (Resource request: <memory:1024, vCores:1> exceeds the available resources of the node and the request cannot be reserved)\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: root.adbs20.e11920598\n",
      "\t start time: 1591453620331\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://c100.local:8088/proxy/application_1591257126602_0273/\n",
      "\t user: e11920598\n",
      "20/06/06 16:27:02 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:02 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:03 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:03 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:04 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:04 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:05 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:05 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:06 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:06 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:07 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:07 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:08 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:08 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:09 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:09 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:10 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:10 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:11 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:11 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:12 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:12 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:13 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:13 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:14 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:14 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:15 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:15 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:16 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:16 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:17 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:17 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:18 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:18 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:19 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:19 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:20 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:20 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:21 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:21 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> c100.local, PROXY_URI_BASES -> http://c100.local:8088/proxy/application_1591257126602_0273), /proxy/application_1591257126602_0273\n",
      "20/06/06 16:27:21 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "20/06/06 16:27:21 INFO yarn.Client: Application report for application_1591257126602_0273 (state: RUNNING)\n",
      "20/06/06 16:27:21 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 10.7.0.105\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: root.adbs20.e11920598\n",
      "\t start time: 1591453620331\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://c100.local:8088/proxy/application_1591257126602_0273/\n",
      "\t user: e11920598\n",
      "20/06/06 16:27:21 INFO cluster.YarnClientSchedulerBackend: Application application_1591257126602_0273 has started running.\n",
      "20/06/06 16:27:21 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1591257126602_0273 and attemptId None\n",
      "20/06/06 16:27:21 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34996.\n",
      "20/06/06 16:27:21 INFO netty.NettyBlockTransferService: Server created on c100.local:34996\n",
      "20/06/06 16:27:21 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/06/06 16:27:21 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c100.local, 34996, None)\n",
      "20/06/06 16:27:21 INFO storage.BlockManagerMasterEndpoint: Registering block manager c100.local:34996 with 366.3 MB RAM, BlockManagerId(driver, c100.local, 34996, None)\n",
      "20/06/06 16:27:21 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c100.local, 34996, None)\n",
      "20/06/06 16:27:21 INFO storage.BlockManager: external shuffle service port = 7337\n",
      "20/06/06 16:27:21 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, c100.local, 34996, None)\n",
      "20/06/06 16:27:21 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "20/06/06 16:27:21 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7456a65c{/metrics/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:22 INFO scheduler.EventLoggingListener: Logging events to hdfs://nameservice1/user/spark/spark2ApplicationHistory/application_1591257126602_0273\n",
      "20/06/06 16:27:22 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/06/06 16:27:22 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "20/06/06 16:27:22 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.NavigatorAppListener\n",
      "20/06/06 16:27:22 INFO logging.DriverLogger$DfsAsyncWriter: Started driver log file sync to: /user/spark/driverLogs/application_1591257126602_0273_driver.log\n",
      "20/06/06 16:27:22 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\n",
      "20/06/06 16:27:22 INFO internal.SharedState: loading hive config file: file:/etc/hive/conf.cloudera.hive/hive-site.xml\n",
      "20/06/06 16:27:22 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').\n",
      "20/06/06 16:27:22 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c778e6b{/SQL,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2162b2dc{/SQL/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bfd4ed1{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16bb0f17{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16041e64{/static/sql,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/06/06 16:27:23 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:23 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/06/06 16:27:23 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 300.5 KB, free 366.0 MB)\n",
      "20/06/06 16:27:23 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.5 KB, free 366.0 MB)\n",
      "20/06/06 16:27:23 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c100.local:34996 (size: 30.5 KB, free: 366.3 MB)\n",
      "20/06/06 16:27:23 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "20/06/06 16:27:24 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:24 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "20/06/06 16:27:24 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Got job 0 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:153)\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "20/06/06 16:27:24 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.0 KB, free 366.0 MB)\n",
      "20/06/06 16:27:24 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 366.0 MB)\n",
      "20/06/06 16:27:24 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on c100.local:34996 (size: 4.4 KB, free: 366.3 MB)\n",
      "20/06/06 16:27:24 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1164\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "20/06/06 16:27:24 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\n",
      "20/06/06 16:27:25 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:25 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:26 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:27 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:28 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:29 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:30 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:31 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:32 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:33 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:34 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:35 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:36 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:37 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:38 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:39 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:40 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:40 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:27:41 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:42 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:43 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:44 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:45 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:46 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:47 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:48 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:49 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:50 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:51 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:52 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:53 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:54 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:55 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:55 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:27:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:58 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:59 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:00 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:01 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:02 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:03 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:04 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:05 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:06 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:07 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:08 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:09 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:10 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:10 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:28:11 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:12 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:13 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:14 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:15 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:16 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:17 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:18 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:19 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:20 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:21 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:22 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:23 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:24 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:25 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:25 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:28:26 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:27 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:28 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:29 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:30 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:31 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:32 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:33 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:34 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:35 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:36 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:37 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:38 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:39 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:40 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:40 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:28:41 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:42 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:43 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:44 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:45 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:46 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:47 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:48 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:49 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:50 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:51 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:52 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:53 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:54 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:55 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:55 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:28:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:58 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:59 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:00 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:01 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:02 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:03 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:04 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:05 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:06 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:07 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:08 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:09 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:10 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:10 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:29:11 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:12 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:13 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:14 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:15 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:16 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:17 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:18 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:19 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:20 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:21 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:22 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:23 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:24 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:25 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:25 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:29:26 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:27 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:28 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:29 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:30 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:31 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:32 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:33 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:34 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:35 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:36 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:37 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:38 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:39 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:40 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:40 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:29:41 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:42 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:42 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.7.0.104:46450) with ID 1\n",
      "20/06/06 16:29:42 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 1)\n",
      "20/06/06 16:29:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, c104.local, executor 1, partition 0, NODE_LOCAL, 7925 bytes)\n",
      "20/06/06 16:29:42 INFO storage.BlockManagerMasterEndpoint: Registering block manager c104.local:35596 with 4.1 GB RAM, BlockManagerId(1, c104.local, 35596, None)\n",
      "20/06/06 16:29:43 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on c104.local:35596 (size: 4.4 KB, free: 4.1 GB)\n",
      "20/06/06 16:29:43 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c104.local:35596 (size: 30.5 KB, free: 4.1 GB)\n",
      "20/06/06 16:29:45 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3257 ms on c104.local (executor 1) (1/1)\n",
      "20/06/06 16:29:45 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/06/06 16:29:45 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 57753\n",
      "20/06/06 16:29:45 INFO scheduler.DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:153) finished in 140.979 s\n",
      "20/06/06 16:29:45 INFO scheduler.DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:153, took 141.059762 s\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adbs20/e11920598/git/twitter/demo.py\", line 18, in <module>\n",
      "    preproc = twitter_preproc(spark, sc, train)\n",
      "  File \"/home/adbs20/e11920598/git/twitter/twitter_preproc.py\", line 12, in __init__\n",
      "    self.inputData = inputRDD.toDF()    \n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 58, in toDF\n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 757, in createDataFrame\n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 401, in _createFromRDD\n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 381, in _inferSchema\n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1062, in _infer_schema\n",
      "TypeError: Can not infer schema for type: <class 'str'>\n",
      "20/06/06 16:29:45 INFO spark.SparkContext: Invoking stop() from shutdown hook\n",
      "20/06/06 16:29:46 INFO server.AbstractConnector: Stopped Spark@6b588ecd{HTTP/1.1,[http/1.1]}{0.0.0.0:4050}\n",
      "20/06/06 16:29:46 INFO ui.SparkUI: Stopped Spark web UI at http://c100.local:4050\n",
      "20/06/06 16:29:46 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "20/06/06 16:29:46 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\n",
      "20/06/06 16:29:46 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "20/06/06 16:29:46 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\n",
      "(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\n",
      "20/06/06 16:29:46 INFO cluster.YarnClientSchedulerBackend: Stopped\n",
      "20/06/06 16:29:50 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/06/06 16:29:50 INFO memory.MemoryStore: MemoryStore cleared\n",
      "20/06/06 16:29:50 INFO storage.BlockManager: BlockManager stopped\n",
      "20/06/06 16:29:50 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/06/06 16:29:50 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/06/06 16:29:50 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "20/06/06 16:29:50 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "20/06/06 16:29:50 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7dd63ea1-eb33-499d-b26d-5c373fb56172\n",
      "20/06/06 16:29:50 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-95acd95f-2bb4-4950-bc77-662ca74baeab\n",
      "20/06/06 16:29:50 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-95acd95f-2bb4-4950-bc77-662ca74baeab/pyspark-d6754d16-25f7-4661-bee2-34926c8e0074\n"
     ]
    }
   ],
   "source": [
    "### rather use it on the command line than here\n",
    "#! spark-submit --num-executors=4 --total-executor-cores 16 --executor-memory=8G demo.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "column_names = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
    "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
    "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\\\n",
    "               \"engaging_user_account_creation\", \"engaged_follows_engaging\", \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
    "\n",
    "SCHEMA = StructType([\n",
    "                StructField(\"text_tokens\", StringType()),\n",
    "                StructField(\"hashtags\", StringType()),\n",
    "                StructField(\"tweet_id\", StringType()),\n",
    "                StructField(\"present_media\", StringType()),\n",
    "                StructField(\"present_links\", StringType()),\n",
    "                StructField(\"present_domains\", StringType()),\n",
    "                StructField(\"tweet_type\", StringType()),\n",
    "                StructField(\"language\", StringType()),\n",
    "                StructField(\"tweet_timestamp\", LongType()),\n",
    "                StructField(\"engaged_with_user_id\", StringType()),\n",
    "                StructField(\"engaged_with_user_follower_count\", LongType()),\n",
    "                StructField(\"engaged_with_user_following_count\", LongType()),\n",
    "                StructField(\"engaged_with_user_is_verified\", BooleanType()),\n",
    "                StructField(\"engaged_with_user_account_creation\", LongType()),\n",
    "                StructField(\"engaging_user_id\", StringType()),\n",
    "                StructField(\"engaging_user_follower_count\", LongType()),\n",
    "                StructField(\"engaging_user_following_count\", LongType()),\n",
    "                StructField(\"engaging_user_is_verified\", BooleanType()),\n",
    "                StructField(\"engaging_user_account_creation\", LongType()),\n",
    "                StructField(\"engaged_follows_engaging\", BooleanType()),\n",
    "                StructField(\"reply_timestamp\", LongType()),\n",
    "                StructField(\"retweet_timestamp\", LongType()),\n",
    "                StructField(\"retweet_with_comment_timestamp\", LongType()),\n",
    "                StructField(\"like_timestamp\", LongType())       \n",
    "                                ])\n",
    "\n",
    "len(column_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
