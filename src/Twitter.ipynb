{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter RecSys Challenge 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from twitter_preproc import twitter_preproc\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"ChiSquareSpark\").getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproc Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting twitter_preproc.py\n"
     ]
    }
   ],
   "source": [
    "%%file twitter_preproc.py\n",
    "\n",
    "class twitter_preproc:\n",
    "\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    \n",
    "    \n",
    "    def __init__(self, spark:SparkSession, sc:SparkContext, inputFile:str, colnames:str, SCHEMA, seed:int=123, MF:bool=False):\n",
    "        self.sc = sc\n",
    "        #inputRDD = sc.textFile(inputFile)\n",
    "        #self.inputData = spark.read.option(\"sep\", \"\\x01\").csv(inputFile)\n",
    "        self.inputData = spark.read.csv(path=inputFile, sep=\"\\x01\", header=False, schema=SCHEMA)\n",
    "        if MF=\n",
    "        self._preprocess(seed)\n",
    "        #self.inputData = spark.createDataFrame(inputRDD, sep=\"\\x01\", schema=SCHEMA)    \n",
    "    \n",
    "    def getDF(self):\n",
    "        return self.outputDF\n",
    "    \n",
    "    def _preprocessMF(self):\n",
    "        outputDF = self.inputData\n",
    "        \n",
    "        outputDF = outputDF.select([\"tweet_id\",\"engaged_user_id\",\"engaged_with_user_id\",\n",
    "                                    \"retweet_timestamp\",\"reply_timestamp\",\n",
    "                                    \"retweet_with_comment_timestamp\",\"like_timestamp\"])\n",
    "        return outputDF\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _preprocess(self, seed):\n",
    "        from pyspark.ml.feature import RegexTokenizer, OneHotEncoderEstimator, StringIndexer\n",
    "        \n",
    "        outputDF = self.inputData\n",
    "        \n",
    "        # Drop unnecessary cols\n",
    "        ### drop ids for classification\n",
    "        outputDF = outputDF.drop(\"tweet_id\").drop(\"engaged_user_id\").drop(\"engaged_with_user_id\")\n",
    "        #\n",
    "        \n",
    "        # Split the text tokens to valid format\n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"text_tokens\",outputCol=\"vector\", pattern=\"\\t\")\n",
    "        outputDF = regexTokenizer.transform(outputDF)\n",
    "        outputDF = outputDF.drop(\"text_tokens\").withColumnRenamed(\"vector\", \"text_tokens\")\n",
    "        \n",
    "        regexTokenizer = RegexTokenizer(inputCol=\"present_media\", outputCol=\"media_list\")\n",
    "        outputDF = regexTokenizer.transform(regexTokenizer)\n",
    "        outputDF = outputDF.drop(\"present_media\").withColumnRenamed(\"vector\", \"text_tokens\")\n",
    "        \n",
    "        # OneHotEncode tweet_type\n",
    "        ## TODO: user_id, engaged_user_id, ...\n",
    "        indexer = StringIndexer(inputCol=\"tweet_type\", outputCol=\"tweet_type_id\")\n",
    "        outputDF = indexer.fit(outputDF).transform(outputDF)\n",
    "        #onehot\n",
    "        encoder = OneHotEncoderEstimator(inputCols=[\"tweet_type_id\"], outputCols=[\"tweet_type_onehot\"])\n",
    "        model = encoder.fit(outputDF)\n",
    "        outputDF = model.transform(outputDF)\n",
    "        # for explainability safe this\n",
    "        self.explainOneHotDF = outputDF.select(\"tweet_type\", \"tweet_type_id\", \"tweet_type_onehot\")\n",
    "        \n",
    "        # tf/idf text_tokens, hashtags, \n",
    "        \n",
    "        \n",
    "        # might not need\n",
    "        # transform boolean to 0-1 column... first one has to change the type in the schema though \n",
    "        #data = data.select(\"engaging_user_is_verified\", \"engaged_with_user_is_verified\", \"engaged_follows_engaging\")\\\n",
    "        #    .replace([\"false\",\"true\"], [\"0\",\"1\"]).show()\n",
    "        \n",
    "        \n",
    "        self.outputDF = outputDF\n",
    "        \n",
    "    '''\n",
    "        returns small dataframe that explains the values of the oneHotEncoder step, this might be needed\n",
    "        for mapping the encodings back to the original values\n",
    "    '''    \n",
    "    def explainOneHot(self):\n",
    "        return self.explainOneHotDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = \"///user/e11920598/traintweet_1000.tsv\"\n",
    "train = \"///tmp/traintweet_1000.tsv\"\n",
    "#train = \"///user/pknees/RSC20/training.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "column_names = [\"text_tokens\", \"hashtags\", \"tweet_id\", \"present_media\", \"present_links\", \"present_domains\",\\\n",
    "                \"tweet_type\", \"language\", \"tweet_timestamp\", \"engaged_with_user_id\", \"engaged_with_user_follower_count\",\\\n",
    "               \"engaged_with_user_following_count\", \"engaged_with_user_is_verified\", \"engaged_with_user_account_creation\",\\\n",
    "               \"engaging_user_id\", \"engaging_user_follower_count\", \"engaging_user_following_count\", \"engaging_user_is_verified\",\\\n",
    "               \"engaging_user_account_creation\", \"engaged_follows_engaging\", \"reply_timestamp\", \"retweet_timestamp\", \"retweet_with_comment_timestamp\", \"like_timestamp\"]\n",
    "SCHEMA = StructType([\n",
    "                StructField(\"text_tokens\", StringType()),\n",
    "                StructField(\"hashtags\", StringType()),\n",
    "                StructField(\"tweet_id\", StringType()),\n",
    "                StructField(\"present_media\", StringType()),\n",
    "                StructField(\"present_links\", StringType()),\n",
    "                StructField(\"present_domains\", StringType()),\n",
    "                StructField(\"tweet_type\", StringType()),\n",
    "                StructField(\"language\", StringType()),\n",
    "                StructField(\"tweet_timestamp\", LongType()),\n",
    "                StructField(\"engaged_with_user_id\", StringType()),\n",
    "                StructField(\"engaged_with_user_follower_count\", LongType()),\n",
    "                StructField(\"engaged_with_user_following_count\", LongType()),\n",
    "                StructField(\"engaged_with_user_is_verified\", BooleanType()),\n",
    "                StructField(\"engaged_with_user_account_creation\", LongType()),\n",
    "                StructField(\"engaging_user_id\", StringType()),\n",
    "                StructField(\"engaging_user_follower_count\", LongType()),\n",
    "                StructField(\"engaging_user_following_count\", LongType()),\n",
    "                StructField(\"engaging_user_is_verified\", BooleanType()),\n",
    "                StructField(\"engaging_user_account_creation\", LongType()),\n",
    "                StructField(\"engaged_follows_engaging\", BooleanType()),\n",
    "                StructField(\"reply_timestamp\", LongType()),\n",
    "                StructField(\"retweet_timestamp\", LongType()),\n",
    "                StructField(\"retweet_with_comment_timestamp\", LongType()),\n",
    "                StructField(\"like_timestamp\", LongType())       \n",
    "                                ])\n",
    "\n",
    "len(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_preproc\n",
    "import importlib\n",
    "importlib.reload(twitter_preproc)\n",
    "from twitter_preproc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+----------+--------------------+---------------+--------------------------------+---------------------------------+-----------------------------+----------------------------------+--------------------+----------------------------+-----------------------------+-------------------------+------------------------------+------------------------+---------------+-----------------+------------------------------+--------------+--------------------+-------------+-----------------+\n",
      "|            hashtags|present_media|       present_links|     present_domains|tweet_type|            language|tweet_timestamp|engaged_with_user_follower_count|engaged_with_user_following_count|engaged_with_user_is_verified|engaged_with_user_account_creation|    engaging_user_id|engaging_user_follower_count|engaging_user_following_count|engaging_user_is_verified|engaging_user_account_creation|engaged_follows_engaging|reply_timestamp|retweet_timestamp|retweet_with_comment_timestamp|like_timestamp|         text_tokens|tweet_type_id|tweet_type_onehot|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+--------------------+---------------+--------------------------------+---------------------------------+-----------------------------+----------------------------------+--------------------+----------------------------+-----------------------------+-------------------------+------------------------------+------------------------+---------------+-----------------+------------------------------+--------------+--------------------+-------------+-----------------+\n",
      "|                null|         null|                null|                null|  TopLevel|22C448FF81263D4BA...|     1581262691|                             986|                             1201|                        false|                        1274269909|00000776B07587ECA...|                          94|                          648|                    false|                    1478011810|                   false|           null|             null|                          null|          null|[101, 1942, 18628...|          0.0|    (2,[0],[1.0])|\n",
      "|83D6C79F5FCEC8D1C...|         null|                null|                null|   Retweet|22C448FF81263D4BA...|     1581497241|                            1225|                              677|                        false|                        1255778244|00000B85AAF7DE172...|                        1139|                           46|                    false|                    1540395738|                    true|           null|       1581497559|                          null|    1581497622|[101, 56898, 137,...|          1.0|    (2,[1],[1.0])|\n",
      "|                null|         null|DDFFB4C01DB85921C...|BE4539C53C53FFABC...|  TopLevel|22C448FF81263D4BA...|     1580978528|                            3016|                             1623|                        false|                        1313450503|00000E0C9B364891C...|                         780|                          440|                    false|                    1432084055|                    true|           null|             null|                          null|    1581060554|[101, 98377, 2262...|          0.0|    (2,[0],[1.0])|\n",
      "|                null|         null|                null|                null|   Retweet|D3164C7FBCF2565DD...|     1581321849|                            2121|                               16|                        false|                        1547717153|00000F04EEDBCF3E1...|                           1|                           45|                    false|                    1534313747|                   false|           null|             null|                          null|    1581328518|[101, 56898, 137,...|          1.0|    (2,[1],[1.0])|\n",
      "|                null|        Photo|                null|                null|  TopLevel|22C448FF81263D4BA...|     1580956787|                          813505|                              200|                         true|                        1476348838|000010088197DA00D...|                         171|                          388|                    false|                    1490166885|                   false|           null|             null|                          null|    1580957807|[101, 100, 119, 6...|          0.0|    (2,[0],[1.0])|\n",
      "+--------------------+-------------+--------------------+--------------------+----------+--------------------+---------------+--------------------------------+---------------------------------+-----------------------------+----------------------------------+--------------------+----------------------------+-----------------------------+-------------------------+------------------------------+------------------------+---------------+-----------------+------------------------------+--------------+--------------------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "preproc = twitter_preproc(spark, sc, train, column_names, SCHEMA)\n",
    "print(preproc.getDF().show(5))\n",
    "#print(preproc.getDF().show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|tweet_timestamp|            language|\n",
      "+---------------+--------------------+\n",
      "|     1581262691|22C448FF81263D4BA...|\n",
      "|     1581497241|22C448FF81263D4BA...|\n",
      "|     1580978528|22C448FF81263D4BA...|\n",
      "|     1581321849|D3164C7FBCF2565DD...|\n",
      "|     1580956787|22C448FF81263D4BA...|\n",
      "|     1581341389|167115458A0DBDFF7...|\n",
      "|     1581004518|ECED8A16BE2A5E887...|\n",
      "|     1581020186|ECED8A16BE2A5E887...|\n",
      "|     1581005860|D3164C7FBCF2565DD...|\n",
      "|     1581103914|D3164C7FBCF2565DD...|\n",
      "|     1581187934|4DC22C3F31C5C4372...|\n",
      "|     1580959419|D3164C7FBCF2565DD...|\n",
      "|     1581043204|D3164C7FBCF2565DD...|\n",
      "|     1581362695|D3164C7FBCF2565DD...|\n",
      "|     1581062422|22C448FF81263D4BA...|\n",
      "|     1581197081|ECED8A16BE2A5E887...|\n",
      "|     1581017422|D3164C7FBCF2565DD...|\n",
      "|     1581521058|D3164C7FBCF2565DD...|\n",
      "|     1581081184|06D61DCBBE938971E...|\n",
      "|     1581260062|06D61DCBBE938971E...|\n",
      "+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "### Tweet-Type OneHotEncodings:\n",
      "+----------+-------------+-----------------+\n",
      "|tweet_type|tweet_type_id|tweet_type_onehot|\n",
      "+----------+-------------+-----------------+\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|     Quote|          2.0|        (2,[],[])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|     Quote|          2.0|        (2,[],[])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|     Quote|          2.0|        (2,[],[])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|  TopLevel|          0.0|    (2,[0],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|\n",
      "|   Retweet|          1.0|    (2,[1],[1.0])|\n",
      "+----------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = preproc.getDF()\n",
    "data.select([\"tweet_timestamp\",\"language\"]).show()\n",
    "#data = data.drop(\"text_tokens\").withColumnRenamed(\"vector\", \"text_tokens\")\n",
    "print(\"### Tweet-Type OneHotEncodings:\")\n",
    "explainonehot = preproc.explainOneHot()\n",
    "explainonehot.show()\n",
    "#data.show()\n",
    "#data.groupBy(\"engaging_user_is_verified\").count().show()\n",
    "#data = data.select(\"engaging_user_is_verified\", \"engaged_with_user_is_verified\", \"engaged_follows_engaging\")\\\n",
    "#    .replace([\"false\",\"true\"], [\"0\",\"1\"])..show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                                 outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "data = preproc.getDF()\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text_tokens\",outputCol=\"vector\", pattern=\"\\t\")\n",
    "tokenized = regexTokenizer.transform(data)\n",
    "\n",
    "tokenized.select(\"vector\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo.py\n"
     ]
    }
   ],
   "source": [
    "%%file demo.py\n",
    "\n",
    "\n",
    "from twitter_preproc import twitter_preproc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"ChiSquareSpark\").getOrCreate()\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "# sample file with 1000 tweets for checking the pipeline\n",
    "train = \"///user/e11920598/traintweet_1000.tsv\"\n",
    "\n",
    "preproc = twitter_preproc(spark, sc, train)\n",
    "print(preproc.getDF().show(5))\n",
    "\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "20/06/06 16:26:47 INFO spark.SparkContext: Running Spark version 2.4.0-cdh6.3.2\n",
      "20/06/06 16:26:47 INFO logging.DriverLogger: Added a local log appender at: /tmp/spark-95acd95f-2bb4-4950-bc77-662ca74baeab/__driver_logs__/driver.log\n",
      "20/06/06 16:26:47 INFO spark.SparkContext: Submitted application: Pipeline\n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: Changing view acls to: e11920598\n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: Changing modify acls to: e11920598\n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "20/06/06 16:26:47 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(e11920598); groups with view permissions: Set(); users  with modify permissions: Set(e11920598); groups with modify permissions: Set()\n",
      "20/06/06 16:26:47 INFO util.Utils: Successfully started service 'sparkDriver' on port 40223.\n",
      "20/06/06 16:26:47 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "20/06/06 16:26:47 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "20/06/06 16:26:47 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/06/06 16:26:47 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/06/06 16:26:47 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b5e841a6-5826-4e85-9c72-4c4474dab04a\n",
      "20/06/06 16:26:47 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/06/06 16:26:47 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "20/06/06 16:26:47 INFO util.log: Logging initialized @2856ms\n",
      "20/06/06 16:26:47 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-09-04T23:11:46+02:00, git hash: 3ce520221d0240229c862b122d2b06c12a625732\n",
      "20/06/06 16:26:47 INFO server.Server: Started @2968ms\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "20/06/06 16:26:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "20/06/06 16:26:47 INFO server.AbstractConnector: Started ServerConnector@6b588ecd{HTTP/1.1,[http/1.1]}{0.0.0.0:4050}\n",
      "20/06/06 16:26:47 INFO util.Utils: Successfully started service 'SparkUI' on port 4050.\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5aad2a0b{/jobs,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b56f2cf{/jobs/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b231bc7{/jobs/job,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7dbdd439{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51d191cb{/stages,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27f5515{/stages/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62fad302{/stages/stage,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54d87832{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@608ed368{/stages/pool,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3f5fde0e{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2904f346{/storage,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e8a07bc{/storage/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28899696{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69678227{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67f7c28e{/environment,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9ae8e2b{/environment/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d7794ae{/executors,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38d4459d{/executors/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@616e13bb{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ccc166b{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d4f23b8{/static,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@787dc45e{/,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2beb43cd{/api,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a626254{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c395428{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:26:47 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://c100.local:4050\n",
      "20/06/06 16:26:48 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:48 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/06/06 16:26:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/06/06 16:26:48 INFO client.RMProxy: Connecting to ResourceManager at c100.local/10.7.0.100:8032\n",
      "20/06/06 16:26:49 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:49 INFO yarn.Client: Requesting a new application from cluster with 18 NodeManagers\n",
      "20/06/06 16:26:49 INFO conf.Configuration: resource-types.xml not found\n",
      "20/06/06 16:26:49 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "20/06/06 16:26:49 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (143360 MB per container)\n",
      "20/06/06 16:26:49 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "20/06/06 16:26:49 INFO yarn.Client: Setting up container launch context for our AM\n",
      "20/06/06 16:26:49 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "20/06/06 16:26:49 INFO yarn.Client: Preparing resources for our AM container\n",
      "20/06/06 16:26:50 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:51 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:51 INFO yarn.Client: Uploading resource file:/tmp/spark-95acd95f-2bb4-4950-bc77-662ca74baeab/__spark_conf__4692943125962393701.zip -> hdfs://nameservice1/user/e11920598/.sparkStaging/application_1591257126602_0273/__spark_conf__.zip\n",
      "20/06/06 16:26:52 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:53 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:54 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:55 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:58 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:26:59 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:00 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: Changing view acls to: e11920598\n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: Changing modify acls to: e11920598\n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "20/06/06 16:27:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(e11920598); groups with view permissions: Set(); users  with modify permissions: Set(e11920598); groups with modify permissions: Set()\n",
      "20/06/06 16:27:00 INFO yarn.Client: Submitting application application_1591257126602_0273 to ResourceManager\n",
      "20/06/06 16:27:00 INFO impl.YarnClientImpl: Submitted application application_1591257126602_0273\n",
      "20/06/06 16:27:01 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:01 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:01 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Sat Jun 06 16:27:01 +0200 2020] Application is added to the scheduler and is not yet activated.  (Resource request: <memory:1024, vCores:1> exceeds the available resources of the node and the request cannot be reserved)\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: root.adbs20.e11920598\n",
      "\t start time: 1591453620331\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://c100.local:8088/proxy/application_1591257126602_0273/\n",
      "\t user: e11920598\n",
      "20/06/06 16:27:02 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:02 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:03 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:03 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:04 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:04 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:05 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:05 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:06 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:06 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:07 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:07 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:08 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:08 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:09 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:09 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:10 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:10 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:11 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:11 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:12 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:12 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:13 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:13 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:14 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:14 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:15 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:15 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:16 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:16 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:17 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:17 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:18 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:18 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:19 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:19 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:20 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:20 INFO yarn.Client: Application report for application_1591257126602_0273 (state: ACCEPTED)\n",
      "20/06/06 16:27:21 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:21 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> c100.local, PROXY_URI_BASES -> http://c100.local:8088/proxy/application_1591257126602_0273), /proxy/application_1591257126602_0273\n",
      "20/06/06 16:27:21 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /jobs, /jobs/json, /jobs/job, /jobs/job/json, /stages, /stages/json, /stages/stage, /stages/stage/json, /stages/pool, /stages/pool/json, /storage, /storage/json, /storage/rdd, /storage/rdd/json, /environment, /environment/json, /executors, /executors/json, /executors/threadDump, /executors/threadDump/json, /static, /, /api, /jobs/job/kill, /stages/stage/kill.\n",
      "20/06/06 16:27:21 INFO yarn.Client: Application report for application_1591257126602_0273 (state: RUNNING)\n",
      "20/06/06 16:27:21 INFO yarn.Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 10.7.0.105\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: root.adbs20.e11920598\n",
      "\t start time: 1591453620331\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://c100.local:8088/proxy/application_1591257126602_0273/\n",
      "\t user: e11920598\n",
      "20/06/06 16:27:21 INFO cluster.YarnClientSchedulerBackend: Application application_1591257126602_0273 has started running.\n",
      "20/06/06 16:27:21 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1591257126602_0273 and attemptId None\n",
      "20/06/06 16:27:21 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34996.\n",
      "20/06/06 16:27:21 INFO netty.NettyBlockTransferService: Server created on c100.local:34996\n",
      "20/06/06 16:27:21 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/06/06 16:27:21 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, c100.local, 34996, None)\n",
      "20/06/06 16:27:21 INFO storage.BlockManagerMasterEndpoint: Registering block manager c100.local:34996 with 366.3 MB RAM, BlockManagerId(driver, c100.local, 34996, None)\n",
      "20/06/06 16:27:21 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, c100.local, 34996, None)\n",
      "20/06/06 16:27:21 INFO storage.BlockManager: external shuffle service port = 7337\n",
      "20/06/06 16:27:21 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, c100.local, 34996, None)\n",
      "20/06/06 16:27:21 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "20/06/06 16:27:21 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7456a65c{/metrics/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:22 INFO scheduler.EventLoggingListener: Logging events to hdfs://nameservice1/user/spark/spark2ApplicationHistory/application_1591257126602_0273\n",
      "20/06/06 16:27:22 INFO util.Utils: Using initial executors = 4, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "20/06/06 16:27:22 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "20/06/06 16:27:22 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.NavigatorAppListener\n",
      "20/06/06 16:27:22 INFO logging.DriverLogger$DfsAsyncWriter: Started driver log file sync to: /user/spark/driverLogs/application_1591257126602_0273_driver.log\n",
      "20/06/06 16:27:22 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\n",
      "20/06/06 16:27:22 INFO internal.SharedState: loading hive config file: file:/etc/hive/conf.cloudera.hive/hive-site.xml\n",
      "20/06/06 16:27:22 INFO internal.SharedState: spark.sql.warehouse.dir is not set, but hive.metastore.warehouse.dir is set. Setting spark.sql.warehouse.dir to the value of hive.metastore.warehouse.dir ('/user/hive/warehouse').\n",
      "20/06/06 16:27:22 INFO internal.SharedState: Warehouse path is '/user/hive/warehouse'.\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3c778e6b{/SQL,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2162b2dc{/SQL/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1bfd4ed1{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16bb0f17{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\n",
      "20/06/06 16:27:22 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@16041e64{/static/sql,null,AVAILABLE,@Spark}\n",
      "20/06/06 16:27:22 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "20/06/06 16:27:23 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:23 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "20/06/06 16:27:23 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 300.5 KB, free 366.0 MB)\n",
      "20/06/06 16:27:23 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 30.5 KB, free 366.0 MB)\n",
      "20/06/06 16:27:23 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c100.local:34996 (size: 30.5 KB, free: 366.3 MB)\n",
      "20/06/06 16:27:23 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "20/06/06 16:27:24 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:24 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "20/06/06 16:27:24 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Got job 0 (runJob at PythonRDD.scala:153) with 1 output partitions\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (runJob at PythonRDD.scala:153)\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "20/06/06 16:27:24 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.0 KB, free 366.0 MB)\n",
      "20/06/06 16:27:24 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.4 KB, free 366.0 MB)\n",
      "20/06/06 16:27:24 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on c100.local:34996 (size: 4.4 KB, free: 366.3 MB)\n",
      "20/06/06 16:27:24 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1164\n",
      "20/06/06 16:27:24 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (PythonRDD[2] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "20/06/06 16:27:24 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\n",
      "20/06/06 16:27:25 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:25 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:26 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:27 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:28 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:29 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:30 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:31 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:32 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:33 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:34 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:35 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:36 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:37 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:38 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:39 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:40 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:40 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:27:41 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:42 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:43 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:44 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:45 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:46 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:47 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:48 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:49 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:50 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:51 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:52 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:53 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:54 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:55 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:55 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:27:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:58 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:27:59 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:00 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:01 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:02 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:03 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:04 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:05 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:06 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:07 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:08 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:09 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:10 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:10 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:28:11 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:12 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:13 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:14 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:15 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:16 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:17 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:18 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:19 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:20 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:21 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:22 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:23 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:24 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:25 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:25 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:28:26 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:27 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:28 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:29 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:30 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:31 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:32 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:33 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:34 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:35 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:36 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:37 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:38 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:39 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:40 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:40 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:28:41 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:42 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:43 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:44 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:45 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:46 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:47 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:48 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:49 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:50 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:51 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:52 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:53 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:54 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:55 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:55 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:28:56 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:57 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:58 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:28:59 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:00 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:01 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:02 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:03 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:04 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:05 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:06 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:07 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:08 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:09 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:10 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:10 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:29:11 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:12 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:13 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:14 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:15 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:16 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:17 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:18 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:19 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:20 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:21 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:22 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:23 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:24 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:25 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:25 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:29:26 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:27 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:28 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:29 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:30 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:31 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:32 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:33 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:34 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:35 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:36 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:37 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:38 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:39 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:40 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:40 WARN cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "20/06/06 16:29:41 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:42 INFO yarn.SparkRackResolver: Got an error when resolving hostNames. Falling back to /default-rack for all\n",
      "20/06/06 16:29:42 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.7.0.104:46450) with ID 1\n",
      "20/06/06 16:29:42 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 1)\n",
      "20/06/06 16:29:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, c104.local, executor 1, partition 0, NODE_LOCAL, 7925 bytes)\n",
      "20/06/06 16:29:42 INFO storage.BlockManagerMasterEndpoint: Registering block manager c104.local:35596 with 4.1 GB RAM, BlockManagerId(1, c104.local, 35596, None)\n",
      "20/06/06 16:29:43 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on c104.local:35596 (size: 4.4 KB, free: 4.1 GB)\n",
      "20/06/06 16:29:43 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on c104.local:35596 (size: 30.5 KB, free: 4.1 GB)\n",
      "20/06/06 16:29:45 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3257 ms on c104.local (executor 1) (1/1)\n",
      "20/06/06 16:29:45 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "20/06/06 16:29:45 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 57753\n",
      "20/06/06 16:29:45 INFO scheduler.DAGScheduler: ResultStage 0 (runJob at PythonRDD.scala:153) finished in 140.979 s\n",
      "20/06/06 16:29:45 INFO scheduler.DAGScheduler: Job 0 finished: runJob at PythonRDD.scala:153, took 141.059762 s\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adbs20/e11920598/git/twitter/demo.py\", line 18, in <module>\n",
      "    preproc = twitter_preproc(spark, sc, train)\n",
      "  File \"/home/adbs20/e11920598/git/twitter/twitter_preproc.py\", line 12, in __init__\n",
      "    self.inputData = inputRDD.toDF()    \n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 58, in toDF\n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 757, in createDataFrame\n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 401, in _createFromRDD\n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 381, in _inferSchema\n",
      "  File \"/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1062, in _infer_schema\n",
      "TypeError: Can not infer schema for type: <class 'str'>\n",
      "20/06/06 16:29:45 INFO spark.SparkContext: Invoking stop() from shutdown hook\n",
      "20/06/06 16:29:46 INFO server.AbstractConnector: Stopped Spark@6b588ecd{HTTP/1.1,[http/1.1]}{0.0.0.0:4050}\n",
      "20/06/06 16:29:46 INFO ui.SparkUI: Stopped Spark web UI at http://c100.local:4050\n",
      "20/06/06 16:29:46 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "20/06/06 16:29:46 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\n",
      "20/06/06 16:29:46 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "20/06/06 16:29:46 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\n",
      "(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\n",
      "20/06/06 16:29:46 INFO cluster.YarnClientSchedulerBackend: Stopped\n",
      "20/06/06 16:29:50 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "20/06/06 16:29:50 INFO memory.MemoryStore: MemoryStore cleared\n",
      "20/06/06 16:29:50 INFO storage.BlockManager: BlockManager stopped\n",
      "20/06/06 16:29:50 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "20/06/06 16:29:50 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "20/06/06 16:29:50 INFO spark.SparkContext: Successfully stopped SparkContext\n",
      "20/06/06 16:29:50 INFO util.ShutdownHookManager: Shutdown hook called\n",
      "20/06/06 16:29:50 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7dd63ea1-eb33-499d-b26d-5c373fb56172\n",
      "20/06/06 16:29:50 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-95acd95f-2bb4-4950-bc77-662ca74baeab\n",
      "20/06/06 16:29:50 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-95acd95f-2bb4-4950-bc77-662ca74baeab/pyspark-d6754d16-25f7-4661-bee2-34926c8e0074\n"
     ]
    }
   ],
   "source": [
    "### rather use it on the command line than here\n",
    "#! spark-submit --num-executors=4 --total-executor-cores 16 --executor-memory=8G demo.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
