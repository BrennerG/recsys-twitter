{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK COLLABORATIVE FILTERING\n",
    "This notebook demonstrates our approach for NNCF on a small sample dataset.  \n",
    "Note that some of the functionality is implemented in other classes:\n",
    "* `twitter_preproc.py` (preprocessing)\n",
    "* `nnpreprocessor.py` (one hot encoding)\n",
    "* `NNCFNet.py` (the neural network class)\n",
    "\n",
    "Our neural network approach for the full dataset is in `nncf-submit.py`. To reproduce our submission attempts use: `spark-submit nncf-submit.py`. (NOTE: As described in our report, this script never actually ran through, due to memory overload exceptions. It is just here for reproduction purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import importlib\n",
    "\n",
    "# Building Spark Context\n",
    "# conf = SparkConf().setAll([('spark.executor.memory', '32g'), ('spark.executor.instances','8'),('spark.executor.cores', '12'), ('spark.driver.memory','64g'), ('spark.driver.memoryOverhead', '64g')])\n",
    "conf = SparkConf()\n",
    "spark = SparkSession.builder.appName(\"nncf_train\").config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_preproc\n",
    "\n",
    "base = \"///tmp/\"\n",
    "one_k = \"traintweet_1000.tsv\"\n",
    "ensemble_train = 'supersecret_ensembletrain5k_bootstrap.tsv'\n",
    "ensemble_test = 'supersecret_test5k_bootstrap.tsv'\n",
    "choice = ensemble_train\n",
    "\n",
    "preproc = twitter_preproc.twitter_preproc(spark, sc, base+choice, MF=True)\n",
    "traindata = preproc.getDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNCF specific preprocessing (essentially onehot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnpreprocessor\n",
    "importlib.reload(nnpreprocessor)\n",
    "\n",
    "nnp = nnpreprocessor.NNPreprocessor()\n",
    "engagement = 'like'\n",
    "tweets, users, target = nnp.nn_preprocess(traindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1\n",
      "epoch  2\n"
     ]
    }
   ],
   "source": [
    "from NNCFNet import Net\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initalize Hyperparameters\n",
    "k = 64\n",
    "n_epochs = 2\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize Neural Network\n",
    "net = Net(users.shape[1], tweets.shape[1], k)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "output = net(users, tweets)\n",
    "\n",
    "# printing aids\n",
    "print_c = 0\n",
    "loss_list = []\n",
    "\n",
    "# Start training\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    print(\"epoch \", epoch+1)\n",
    "\n",
    "    permutation = torch.randperm(users.size()[0])\n",
    "    for i in range(0,users.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x_user = users[indices]\n",
    "        batch_x_tweet = tweets[indices]\n",
    "        batch_y = target[indices]\n",
    "\n",
    "        outputs = net.forward(batch_x_user, batch_x_tweet)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print & track loss\n",
    "        loss_list.append((print_c,float(loss)))\n",
    "        print_c += 1\n",
    "        \n",
    "# plot the loss\n",
    "plot_df = pd.DataFrame(loss_list)\n",
    "ax = plot_df.set_index(0)[1].plot();\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save loss plot\n",
    "fig.savefig('../misc/nncf/' + engagement + '.' + 'k' + str(k) + 'ep' + str(n_epochs) + 'batch' + str(batch_size) + str('lr') + str(learning_rate) + '_adam&bce.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create & format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import numpy as np\n",
    "\n",
    "# get predictions\n",
    "net.eval()\n",
    "prediction = net(users, tweets)\n",
    "p_vec = prediction.detach().numpy().flatten()\n",
    "\n",
    "# scale output\n",
    "scaled = p_vec\n",
    "# scaled = (p_vec - np.min(p_vec))/np.ptp(p_vec)\n",
    "probabilities = [float(x) for x in scaled]\n",
    "\n",
    "# get original order\n",
    "order_df = traindata.withColumn(\"original_order\", monotonically_increasing_id())\n",
    "order_df = order_df.select(\"engaging_user_id\", \"tweet_id\", 'original_order')\n",
    "sorting_tweets = nnp.get_id_indices(order_df, id_column='tweet_id')\n",
    "\n",
    "# rejoin labels\n",
    "result = order_df.join(sorting_tweets, 'tweet_id').sort('original_order').rdd.map(lambda x: (x['engaging_user_id'], x['tweet_id'], probabilities[x['tweet_id_index']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw histogram of predictions\n",
    "hist_df = pd.DataFrame(result.toDF([\"engaging_user_id\", \"tweet_id\", 'target']).collect())[2].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path hdfs://nameservice1/user/e1447920/like.supersecret_ensembletrain5k_bootstrap.csv already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o385.csv.\n: org.apache.spark.sql.AnalysisException: path hdfs://nameservice1/user/e1447920/like.supersecret_ensembletrain5k_bootstrap.csv already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:656)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c6db489631b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengagement\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"engaging_user_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tweet_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\x01\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[1;32m    929\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[0;32m--> 931\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path hdfs://nameservice1/user/e1447920/like.supersecret_ensembletrain5k_bootstrap.csv already exists.;'"
     ]
    }
   ],
   "source": [
    "output_PATH = str(engagement + '.' + choice).replace('tsv', 'csv')\n",
    "result_df = result.toDF([\"engaging_user_id\", \"tweet_id\", 'target'])\n",
    "result_df.repartition(1).write.csv(output_PATH, sep=\"\\x01\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
